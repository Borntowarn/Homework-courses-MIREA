{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:19:11.350013Z","iopub.status.busy":"2023-03-01T11:19:11.349280Z","iopub.status.idle":"2023-03-01T11:19:22.878192Z","shell.execute_reply":"2023-03-01T11:19:22.876973Z","shell.execute_reply.started":"2023-03-01T11:19:11.349970Z"},"id":"khe7vy_ZwLii","trusted":true},"outputs":[],"source":["#!pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:19:26.248097Z","iopub.status.busy":"2023-03-01T11:19:26.246906Z","iopub.status.idle":"2023-03-01T11:19:37.199774Z","shell.execute_reply":"2023-03-01T11:19:37.198744Z","shell.execute_reply.started":"2023-03-01T11:19:26.248053Z"},"id":"khe7vy_ZwLii","trusted":true},"outputs":[],"source":["import os\n","import gc\n","import torch\n","import wandb\n","import pytorch_lightning as pl\n","import torch.nn.functional as F\n","\n","\n","from torch import nn\n","from typing import *\n","from einops import rearrange\n","from tqdm.notebook import tqdm\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from pytorch_lightning.tuner.tuning import Tuner\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","\n","torch.random.manual_seed(0)\n","pl.seed_everything(0)"]},{"cell_type":"markdown","metadata":{},"source":["# Patch embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:33.237360Z","iopub.status.busy":"2023-03-01T11:22:33.236573Z","iopub.status.idle":"2023-03-01T11:22:33.245930Z","shell.execute_reply":"2023-03-01T11:22:33.244756Z","shell.execute_reply.started":"2023-03-01T11:22:33.237322Z"},"trusted":true},"outputs":[],"source":["class PatchEmbedding(nn.Module):\n","    \"\"\" \n","    Image to Patch Embedding\n","    \"\"\"\n","    def __init__(\n","        self,\n","        img_size: int = 224,\n","        patch_size: int = 16,\n","        in_chans: int = 3,\n","        d_model: int = 768\n","    ):\n","        super().__init__()\n","        \n","        self.d_model = d_model\n","        self.in_chans = in_chans\n","        self.img_size = img_size\n","        \n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.patch_embeddings = nn.Conv2d(3, self.d_model, patch_size, patch_size)\n","\n","    def forward(self, image):\n","        b, c, h, w = image.shape\n","        \n","        assert h == self.img_size and w == self.img_size, f'Image size must be {self.img_size}x{self.img_size}'\n","        assert c == self.in_chans, f'Image must have {self.in_chans} channels'\n","        \n","        patches = self.patch_embeddings(image).reshape(b, self.d_model, -1).transpose(1, 2)\n","        \n","        return patches"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:12.554408Z","iopub.status.busy":"2023-02-25T22:20:12.551201Z","iopub.status.idle":"2023-02-25T22:20:12.656371Z","shell.execute_reply":"2023-02-25T22:20:12.655362Z","shell.execute_reply.started":"2023-02-25T22:20:12.554361Z"},"trusted":true},"outputs":[],"source":["x = torch.randn((2, 3, 224, 224))\n","PatchEmbedding()(x).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Residual block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:36.872568Z","iopub.status.busy":"2023-03-01T11:22:36.871891Z","iopub.status.idle":"2023-03-01T11:22:36.878786Z","shell.execute_reply":"2023-03-01T11:22:36.877570Z","shell.execute_reply.started":"2023-03-01T11:22:36.872532Z"},"trusted":true},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    \n","    def __init__(self, func: Optional[Callable] = None) -> None:\n","        super().__init__()\n","        \n","        self.func = func\n","        if not self.func:\n","            self.func = lambda x: x\n","    \n","    def forward(self, x):\n","        x = self.func(x) + x\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:12.669466Z","iopub.status.busy":"2023-02-25T22:20:12.668523Z","iopub.status.idle":"2023-02-25T22:20:12.723587Z","shell.execute_reply":"2023-02-25T22:20:12.722431Z","shell.execute_reply.started":"2023-02-25T22:20:12.669426Z"},"trusted":true},"outputs":[],"source":["x = torch.Tensor([1., 2., 3., 4.])\n","ResidualBlock(lambda x: x**2)(x)"]},{"cell_type":"markdown","metadata":{},"source":["# Multi Head Attention Block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:38.662222Z","iopub.status.busy":"2023-03-01T11:22:38.661314Z","iopub.status.idle":"2023-03-01T11:22:38.671604Z","shell.execute_reply":"2023-03-01T11:22:38.670558Z","shell.execute_reply.started":"2023-03-01T11:22:38.662171Z"},"id":"4QnAW3rSc2OZ","trusted":true},"outputs":[],"source":["class MHABlock(nn.Module):\n","    def __init__(\n","        self,\n","        emb_len: int,\n","        num_heads: int = 8,\n","        attn_drop: float = 0.,\n","        out_drop: float = 0.\n","    ):\n","        super().__init__()\n","        \n","        self.num_heads = num_heads # number of heads\n","        head_emb = emb_len // num_heads # embeddings length after head\n","        self.scale = head_emb ** -0.5 # scale param for decrease dispersion\n","\n","        self.qkv = nn.Linear(emb_len, emb_len * 3, bias=False)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        \n","        self.out = nn.Sequential(\n","            nn.Linear(emb_len, emb_len),\n","            nn.Dropout(out_drop)\n","        )\n","        \n","\n","    def forward(self, x):\n","        \n","        QKV = self.qkv(x)\n","        \"\"\"\n","        b - batch\n","        l - sequence length (number of patches)\n","        n - 3 (Q K V)\n","        h - num heads\n","        hl - seq length after attention\n","        \"\"\"\n","        Q, K, V = rearrange(QKV, 'b l (n h hl) -> n b h l hl', n = 3, h = self.num_heads)\n","\n","        attention = F.softmax(torch.einsum('bhqo, bhko -> bhqk', Q, K) / self.scale, dim=-1)\n","        attention = self.attn_drop(attention)\n","        attention = attention @ V\n","        attention = rearrange(attention, 'b h l hl -> b l (h hl)')\n","        \n","        out = self.out(attention)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:12.738818Z","iopub.status.busy":"2023-02-25T22:20:12.738356Z","iopub.status.idle":"2023-02-25T22:20:13.016589Z","shell.execute_reply":"2023-02-25T22:20:13.015446Z","shell.execute_reply.started":"2023-02-25T22:20:12.738780Z"},"trusted":true},"outputs":[],"source":["x = torch.randn((5, 197, 768))\n","MHABlock(768)(x).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Feed forward block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:42.118370Z","iopub.status.busy":"2023-03-01T11:22:42.118011Z","iopub.status.idle":"2023-03-01T11:22:42.126547Z","shell.execute_reply":"2023-03-01T11:22:42.125416Z","shell.execute_reply.started":"2023-03-01T11:22:42.118338Z"},"id":"VPQts2WWdeYQ","trusted":true},"outputs":[],"source":["class FeedForwardBlock(nn.Module):\n","    def __init__(\n","        self, \n","        in_features: int, \n","        mlp_ratio: int = 4,\n","        hidden_features: Optional[int] = None, \n","        out_features: Optional[int] = None, \n","        drop_rate: float = 0.\n","    ):\n","        super().__init__()\n","        \n","        if not hidden_features:\n","            hidden_features = in_features * mlp_ratio\n","        if not out_features:\n","            out_features = in_features\n","\n","        self.linears = nn.Sequential(\n","            nn.Linear(in_features, hidden_features),\n","            nn.GELU(),\n","            nn.Dropout(drop_rate),\n","            nn.Linear(hidden_features, out_features),\n","        )\n","\n","    def forward(self, x):\n","        x = self.linears(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:13.028590Z","iopub.status.busy":"2023-02-25T22:20:13.027706Z","iopub.status.idle":"2023-02-25T22:20:13.110513Z","shell.execute_reply":"2023-02-25T22:20:13.109343Z","shell.execute_reply.started":"2023-02-25T22:20:13.028550Z"},"id":"LFxxcPoMf7IW","outputId":"c1f44b38-7ec4-4e93-bdb5-a4db4e116e90","trusted":true},"outputs":[],"source":["x = torch.randn(1, 197, 768)\n","FeedForwardBlock(768)(x).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:44.098252Z","iopub.status.busy":"2023-03-01T11:22:44.097881Z","iopub.status.idle":"2023-03-01T11:22:44.106357Z","shell.execute_reply":"2023-03-01T11:22:44.105087Z","shell.execute_reply.started":"2023-03-01T11:22:44.098218Z"},"trusted":true},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    def __init__(\n","        self, \n","        emb_len: int, \n","        num_heads: int = 8, \n","        mlp_ratio: int = 4, \n","        drop_rate: float =0.\n","    ):\n","        super().__init__()\n","\n","        self.first_residual = ResidualBlock(\n","            nn.Sequential(\n","                nn.LayerNorm(emb_len),\n","                MHABlock(emb_len, num_heads, drop_rate, drop_rate),\n","                nn.Dropout(drop_rate)\n","            )\n","        )\n","        \n","        self.second_residual = ResidualBlock(\n","            nn.Sequential(\n","                nn.LayerNorm(emb_len),\n","                FeedForwardBlock(emb_len, mlp_ratio),\n","                nn.Dropout(drop_rate)\n","            )\n","        )           \n","\n","    def forward(self, x):\n","        \n","        x = self.first_residual(x)\n","        x = self.second_residual(x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:13.122722Z","iopub.status.busy":"2023-02-25T22:20:13.122328Z","iopub.status.idle":"2023-02-25T22:20:13.243942Z","shell.execute_reply":"2023-02-25T22:20:13.242889Z","shell.execute_reply.started":"2023-02-25T22:20:13.122683Z"},"id":"3aMihgfEhyql","outputId":"993524e8-c7eb-4b38-802e-557e89ba8285","trusted":true},"outputs":[],"source":["x = torch.randn(1, 197, 768)\n","block = EncoderBlock(768, 12)\n","out = block(x)\n","out.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Transformer class. Stack of EncoderBlocks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:46.869672Z","iopub.status.busy":"2023-03-01T11:22:46.868444Z","iopub.status.idle":"2023-03-01T11:22:46.876939Z","shell.execute_reply":"2023-03-01T11:22:46.875692Z","shell.execute_reply.started":"2023-03-01T11:22:46.869615Z"},"id":"b1uO18VTwLil","trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(\n","        self, \n","        num_layers: int, \n","        emb_len: int, \n","        num_heads: int = 12,\n","        mlp_ratio: int = 4,\n","        drop_rate: float = 0.\n","    ):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([\n","            EncoderBlock(emb_len, num_heads, mlp_ratio, drop_rate)\n","            for i in range(num_layers)])\n","\n","    def forward(self, x):\n","        for block in self.blocks:\n","            x = block(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:13.260468Z","iopub.status.busy":"2023-02-25T22:20:13.260144Z","iopub.status.idle":"2023-02-25T22:20:14.615542Z","shell.execute_reply":"2023-02-25T22:20:14.614352Z","shell.execute_reply.started":"2023-02-25T22:20:13.260439Z"},"id":"hIfp984oiBqc","outputId":"704c9a49-92d6-4859-9f61-06555bf89579","trusted":true},"outputs":[],"source":["x = torch.randn(1, 197, 768)\n","Transformer(12, 768)(x).shape"]},{"cell_type":"markdown","metadata":{},"source":["# Vision Transformer model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:48.878474Z","iopub.status.busy":"2023-03-01T11:22:48.877941Z","iopub.status.idle":"2023-03-01T11:22:48.898011Z","shell.execute_reply":"2023-03-01T11:22:48.896887Z","shell.execute_reply.started":"2023-03-01T11:22:48.878434Z"},"trusted":true},"outputs":[],"source":["class ViT(pl.LightningModule):\n","  \n","    def __init__(\n","        self,\n","        img_size: int = 224,\n","        patch_size: int = 16,\n","        in_chans: int = 3,\n","        num_classes: int = 1000,\n","        emb_len: int = 768,\n","        num_layers: int = 12,\n","        num_heads: int = 12,\n","        mlp_ratio: int = 4,\n","        drop_rate: int = 0.,\n","        loss_func = None\n","    ):\n","        super(ViT, self).__init__()\n","        self.save_hyperparameters(ignore=['loss_func'])\n","        \n","        # Path Embeddings, CLS Token, Position Encoding\n","        self.patch_embeddings = PatchEmbedding(img_size, patch_size, in_chans, emb_len)\n","        self.cls_token = nn.Parameter(torch.randn((1, 1, emb_len)))\n","        self.pos_encodings = nn.Parameter(torch.randn((self.patch_embeddings.num_patches + 1, emb_len)))\n","\n","        # Transformer Encoder\n","        self.transformer = Transformer(num_layers, emb_len, num_heads, mlp_ratio, drop_rate)\n","\n","        # Classifier\n","        self.classifier = nn.Linear(emb_len, num_classes)\n","        \n","        self.loss_func = loss_func\n","\n","\n","    def forward(self, x):\n","        # Path Embeddings, CLS Token, Position Encoding\n","        b, c, h, w = x.shape\n","        \n","        cls_tokens = self.cls_token.expand(b, -1, -1)\n","        x = self.pos_encodings + torch.cat((cls_tokens, self.patch_embeddings(x)), dim = 1)\n","\n","        # Transformer Encoder\n","        x = self.transformer(x)[:, 0, :].squeeze(1)\n","\n","        # Classifier\n","        predictions = self.classifier(x)\n","\n","        return predictions\n","\n","\n","    # Настраиваются параметры обучения\n","    def training_step(self, batch, batch_idx):\n","        data, targets = batch\n","        \n","        logits = self(data)\n","        \n","        loss = self.loss_func(logits, targets)\n","        accuracy = torch.sum(logits.argmax(-1) == targets) / len(logits)\n","        \n","        lr = self.lr_schedulers().get_last_lr()[-1]\n","        self.log('loss', loss, on_epoch=True, on_step=False)\n","        self.log('acc', accuracy, on_epoch=True, on_step=False)\n","        self.log('Lr', lr, on_epoch=True, on_step=False)\n","        \n","        output = {\n","            'loss': loss,\n","            'acc': accuracy,\n","            'lr': lr\n","        }\n","        \n","        return output\n","\n","\n","    # Настраиваются параметры тестирования\n","    def test_step(self, batch, batch_idx):\n","        data, targets = batch\n","        logits = self(data)\n","        \n","        loss = self.loss_func(logits, targets)\n","        accuracy = torch.sum(logits.argmax(-1) == targets) / len(logits)\n","    \n","        self.log('Test acc', accuracy, prog_bar=True) \n","        output = {\n","            'loss': loss,\n","            'acc': accuracy\n","        }\n","        return output\n","\n","\n","    # Конфигурируется оптимизатор\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","            optimizer, \n","            max_lr = 0.0004,\n","            total_steps = self.trainer.max_epochs * self.trainer.datamodule.len_train_dataloader,\n","            pct_start = 0.05)\n","        \n","        config = {\n","            'optimizer': optimizer,\n","            'lr_scheduler': {\n","                'scheduler': scheduler,\n","                'interval': 'step'\n","            }\n","        }\n","        \n","        return config\n","    \n","    def training_epoch_end(self, outputs) -> None:\n","        loss = sum(output['loss'] for output in outputs) / len(outputs)\n","        print(f'Эпоха {self.current_epoch}, loss = {loss}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-25T22:20:14.638872Z","iopub.status.busy":"2023-02-25T22:20:14.638480Z","iopub.status.idle":"2023-02-25T22:20:20.585481Z","shell.execute_reply":"2023-02-25T22:20:20.584287Z","shell.execute_reply.started":"2023-02-25T22:20:14.638827Z"},"trusted":true},"outputs":[],"source":["x = torch.randn(10, 3, 224, 224)\n","vit = ViT(num_classes=15)\n","out = vit(x)\n","out.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:52.137889Z","iopub.status.busy":"2023-03-01T11:22:52.137217Z","iopub.status.idle":"2023-03-01T11:22:52.148131Z","shell.execute_reply":"2023-03-01T11:22:52.147081Z","shell.execute_reply.started":"2023-03-01T11:22:52.137854Z"},"trusted":true},"outputs":[],"source":["class DataModule(pl.LightningDataModule):\n","    \n","    \n","    def __init__(\n","        self,\n","        root_dir: str,\n","        train_folder: str,\n","        test_folder: str,\n","        batch_size: int\n","    ) -> None:\n","        super(pl.LightningDataModule, self).__init__()\n","        \n","        self.train_dir = os.path.join(root_dir, train_folder)\n","        self.test_dir = os.path.join(root_dir, test_folder)\n","        \n","        self.transform = transforms.Compose([\n","            transforms.Resize((224, 224)), \n","            transforms.ToTensor()])\n","        \n","        self.batch_size = batch_size\n","    \n","    \n","    def setup(self, stage: Optional[str] = None) -> None:\n","        if stage == 'fit':\n","            self.train_data = datasets.ImageFolder(self.train_dir, self.transform)\n","            self.len_train_dataloader = len(self.train_data) // self.batch_size\n","        if stage == 'test':\n","            self.test_data = datasets.ImageFolder(self.test_dir, self.transform)\n","    \n","    \n","    def train_dataloader(self) -> DataLoader:\n","        return DataLoader(self.train_data, self.batch_size, True, drop_last=True)\n","\n","    \n","    def test_dataloader(self) -> DataLoader:\n","        return DataLoader(self.test_data, self.batch_size, False, drop_last=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:22:55.196228Z","iopub.status.busy":"2023-03-01T11:22:55.195863Z","iopub.status.idle":"2023-03-01T11:22:55.201912Z","shell.execute_reply":"2023-03-01T11:22:55.200537Z","shell.execute_reply.started":"2023-03-01T11:22:55.196197Z"},"trusted":true},"outputs":[],"source":["patch_size = 16\n","num_heads = 8\n","num_layers = 8\n","emb_len = 384\n","drop_rate = 0.07\n","max_lr = 0.0004\n","epochs = 200\n","root_dir = '../input/vegetable-image-dataset/Vegetable Images'"]},{"cell_type":"markdown","metadata":{},"source":["# Init modules"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:45:01.041729Z","iopub.status.busy":"2023-03-01T11:45:01.040656Z","iopub.status.idle":"2023-03-01T11:45:01.372474Z","shell.execute_reply":"2023-03-01T11:45:01.371414Z","shell.execute_reply.started":"2023-03-01T11:45:01.041685Z"},"trusted":true},"outputs":[],"source":["sweep_configuration = {\n","    'method': 'grid',\n","    'name': 'sweep',\n","    'metric': {\n","        'goal': 'maximize', \n","        'name': 'Test acc'\n","        },\n","    'parameters': {\n","        'patch_size': {'values': [14, 16]},\n","        'num_heads': {'values': [8, 12]}\n","     }\n","}\n","\n","sweep_id = wandb.sweep(sweep=sweep_configuration, project = \"First ViT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:45:04.376965Z","iopub.status.busy":"2023-03-01T11:45:04.376006Z","iopub.status.idle":"2023-03-01T11:45:04.389099Z","shell.execute_reply":"2023-03-01T11:45:04.387845Z","shell.execute_reply.started":"2023-03-01T11:45:04.376912Z"},"trusted":true},"outputs":[],"source":["def train():\n","    with wandb.init() as run:\n","        config = wandb.config\n","        run.name = f\"P_{config.patch_size} \\\n","                     -H_{config.num_heads} \\\n","                     -L_{num_layers} \\\n","                     -E_{emb_len} \\\n","                     -D_{drop_rate} \\\n","                     -LR_{max_lr} \\\n","                     -Epochs_{epochs}\"\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        try:\n","            pl.seed_everything(seed=0)\n","            \n","            print('Initializing modules...')\n","            \n","            datamodule = DataModule(root_dir, 'validation', 'test', 64)\n","            model = ViT(\n","                patch_size=config.patch_size,\n","                num_heads=config.num_heads,\n","                num_layers=num_layers,\n","                emb_len=emb_len,\n","                num_classes=15,\n","                drop_rate=drop_rate,\n","                loss_func=nn.CrossEntropyLoss()\n","            )\n","            wandb_logger = WandbLogger(log_model = True)\n","            wandb_logger.watch(model, log = 'all', log_freq = 100)\n","            trainer = pl.Trainer(\n","                accelerator = 'gpu',\n","                max_epochs = epochs,\n","                default_root_dir = './lightning',\n","                logger = wandb_logger\n","            )\n","            \n","            print('Initializing successful...')\n","            \n","            print('Start training')\n","            trainer.fit(model, datamodule)\n","            print('Training is finished')\n","            \n","            print('Start testing')\n","            trainer.test(model, datamodule)\n","            print('Testing is finished')\n","            \n","        except Exception as e:\n","            print(e)\n","\n","        del model\n","        del datamodule\n","        del wandb_logger\n","        del trainer\n","            \n","        gc.collect()\n","        torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T11:45:10.507867Z","iopub.status.busy":"2023-03-01T11:45:10.507267Z","iopub.status.idle":"2023-03-01T11:45:48.682199Z","shell.execute_reply":"2023-03-01T11:45:48.681133Z","shell.execute_reply.started":"2023-03-01T11:45:10.507826Z"},"trusted":true},"outputs":[],"source":["wandb.agent(sweep_id, function=train, count=10)"]},{"cell_type":"markdown","metadata":{},"source":["![Image-alt](./Sweeps.jpg)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"vscode":{"interpreter":{"hash":"cffdd22708c8f24895f497a03a7b67c0092c0fcb40692f19cd97059e00134830"}}},"nbformat":4,"nbformat_minor":4}
