{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "khe7vy_ZwLii"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patch embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\" \n",
        "    Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, d_model=768):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.in_chans = in_chans\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embeddings = nn.Conv2d(3, self.d_model, 16, 16)\n",
        "\n",
        "    def forward(self, image):\n",
        "        b, c, h, w = image.shape\n",
        "        \n",
        "        assert h == self.img_size and w == self.img_size, f'Image size must be {self.img_size}x{self.img_size}'\n",
        "        assert c == self.in_chans, f'Image must have {self.in_chans} channels'\n",
        "        \n",
        "        patches = self.patch_embeddings(image).reshape(b, self.d_model, -1).transpose(1, 2)\n",
        "        \n",
        "        return patches\n",
        "    \n",
        "    def forward1(self, image):\n",
        "\n",
        "        B, C, H, W = image.shape\n",
        "        patches = self.patch_embeddings(image).flatten(2).transpose(1,2)\n",
        "        \n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn((2, 3, 224, 224))\n",
        "pe = PatchEmbedding()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, func = None) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.func = func\n",
        "        if not self.func:\n",
        "            self.func = lambda x: x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.func(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 2.,  6., 12., 20.])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.Tensor([1., 2., 3., 4.])\n",
        "ResidualBlock(lambda x: x**2)(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi Head Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4QnAW3rSc2OZ"
      },
      "outputs": [],
      "source": [
        "class MHABlock(nn.Module):\n",
        "    def __init__(self, emb_len, num_heads=8, attn_drop=0., out_drop=0.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_heads = num_heads # number of heads\n",
        "        head_emb = emb_len // num_heads # embeddings length after head\n",
        "        self.scale = head_emb ** -0.5 # scale param for decrease dispersion\n",
        "\n",
        "        self.qkv = nn.Linear(emb_len, emb_len * 3, bias=False)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        \n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(emb_len, emb_len),\n",
        "            nn.Dropout(out_drop)\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        QKV = self.qkv(x)\n",
        "        \"\"\"\n",
        "        b - batch\n",
        "        l - sequence length (number of patches)\n",
        "        n - 3 (Q K V)\n",
        "        h - num heads\n",
        "        hl - seq length after attention\n",
        "        \"\"\"\n",
        "        \n",
        "        Q, K, V = rearrange(QKV, 'b l (n h hl) -> n b h l hl', n = 3, h = self.num_heads)\n",
        "\n",
        "        attention = F.softmax(torch.einsum('bhqo, bhko -> bhqk', Q, K) / self.scale, dim=-1)\n",
        "        attention = self.attn_drop(attention)\n",
        "        attention = attention @ V\n",
        "        attention = rearrange(attention, 'b h l hl -> b l (h hl)')\n",
        "        \n",
        "        out = self.out(attention)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn((5, 197, 768))\n",
        "mha = MHABlock(768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1 = mha.forward(x)\n",
        "x2 = mha.forward1(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(x1 == x2).all()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feed forward block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VPQts2WWdeYQ"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, in_features, mlp_ratio=4, hidden_features=None, out_features=None, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        \n",
        "        if not hidden_features:\n",
        "            hidden_features = in_features * mlp_ratio\n",
        "        if not out_features:\n",
        "            out_features = in_features\n",
        "\n",
        "        self.linears = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_features),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(hidden_features, out_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linears(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFxxcPoMf7IW",
        "outputId": "c1f44b38-7ec4-4e93-bdb5-a4db4e116e90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "FeedForwardBlock(768)(x).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_len, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first_residual = ResidualBlock(\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(emb_len),\n",
        "                MHABlock(emb_len, num_heads, drop_rate, drop_rate),\n",
        "                nn.Dropout(drop_rate)\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        self.second_residual = ResidualBlock(\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(emb_len),\n",
        "                FeedForwardBlock(emb_len, mlp_ratio),\n",
        "                nn.Dropout(drop_rate)\n",
        "            )\n",
        "        )           \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.first_residual(x)\n",
        "        x = self.second_residual(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aMihgfEhyql",
        "outputId": "993524e8-c7eb-4b38-802e-557e89ba8285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "block = EncoderBlock(768, 12)\n",
        "out = block(x)\n",
        "out.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer class. Stack of EncoderBlocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b1uO18VTwLil"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, emb_len, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            EncoderBlock(emb_len, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIfp984oiBqc",
        "outputId": "704c9a49-92d6-4859-9f61-06555bf89579"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "Transformer(12, 768)(x).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Y9gyxdqQeFs6"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "                 emb_len=768, num_layers=12, num_heads=12, mlp_ratio=4, drop_rate=0.,):\n",
        "        super().__init__()\n",
        "\n",
        "        # Присвоение переменных\n",
        "        ...\n",
        "\n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        self.patch_embeddings = PatchEmbedding(img_size, patch_size, in_chans, emb_len)\n",
        "        self.cls_token = nn.Parameter(torch.randn((1, 1, emb_len)))\n",
        "        self.pos_encodings = nn.Parameter(torch.randn((self.patch_embeddings.num_patches + 1, emb_len)))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.transformer = Transformer(num_layers, emb_len, num_heads, mlp_ratio, drop_rate)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(emb_len, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        b, c, h, w = x.shape\n",
        "        \n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = self.pos_encodings + torch.cat((cls_tokens, self.patch_embeddings(x)), dim = 1)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = self.transformer(x)[:, 0, :].squeeze(1)\n",
        "\n",
        "        # Classifier\n",
        "        predictions = self.classifier(x)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 1000])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(10, 3, 224, 224)\n",
        "vit = ViT()\n",
        "out = vit(x)\n",
        "out.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "root_train = './dataset/validation/'\n",
        "root_test = './dataset/test/'\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
        "\n",
        "train_data = datasets.ImageFolder(root_train, transform)\n",
        "test_data = datasets.ImageFolder(root_test, transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, 10, True)\n",
        "test_loader = DataLoader(test_data, 10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x = torch.randn((100, 3, 224, 224))\n",
        "# y = torch.randint(low=0, high=10, size=(1, 100)).squeeze(0)\n",
        "# x_y = TensorDataset(x, y)\n",
        "# loader = DataLoader(x_y, 10, True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        lossfunc: nn.Module,\n",
        "        epochs: int,\n",
        "        device: str = 'cuda'\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.dataloader = dataloader\n",
        "        self.lossfunc = lossfunc\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(device)\n",
        "    \n",
        "    \n",
        "    def train(self) -> nn.Module:\n",
        "        self.model.train()\n",
        "        self.model = self.model.to(self.device)\n",
        "        \n",
        "        for epoch in tqdm(range(1, self.epochs + 1), total=self.epochs):\n",
        "            outputs = []\n",
        "            for (data, targets) in tqdm(self.dataloader, total=len(self.dataloader)):\n",
        "                \n",
        "                loss, acc = self._forward(data, targets)\n",
        "                self._backward(loss)\n",
        "                \n",
        "                outputs.append([loss, acc])\n",
        "            \n",
        "            outputs = torch.Tensor(outputs)\n",
        "            print(f'Эпоха {epoch}: ',outputs.mean(dim=0))\n",
        "        \n",
        "        return self.model\n",
        "\n",
        "\n",
        "    def _forward(self, data: torch.Tensor, targets: torch.Tensor):\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        data = data.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "        \n",
        "        logits = self.model(data)\n",
        "        \n",
        "        loss = self.lossfunc(logits, targets)\n",
        "        acc = torch.sum(logits.argmax(-1) == targets) / len(logits)\n",
        "        \n",
        "        return loss, acc\n",
        "    \n",
        "\n",
        "\n",
        "    def _backward(self, loss: torch.Tensor) -> None:\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tester:\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        device: str = 'cuda'\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.device = torch.device(device)\n",
        "    \n",
        "    \n",
        "    def test(self) -> nn.Module:\n",
        "        self.model.eval()\n",
        "        self.model = self.model.to(self.device)\n",
        "        \n",
        "        outputs = []\n",
        "        for (data, targets) in tqdm(self.dataloader, total=len(self.dataloader)):\n",
        "            \n",
        "            acc = self._forward(data, targets)\n",
        "            outputs.append([acc])\n",
        "        \n",
        "        outputs = torch.Tensor(outputs)\n",
        "        print('Accuracy: ',outputs.mean(dim=0).item())\n",
        "        \n",
        "\n",
        "    def _forward(self, data: torch.Tensor, targets: torch.Tensor):\n",
        "        data = data.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "        \n",
        "        logits = self.model(data)\n",
        "        acc = torch.sum(logits.argmax(-1) == targets) / len(logits)\n",
        "        \n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = {}\n",
        "scores = {}\n",
        "def train(epochs, loader):\n",
        "\n",
        "    model = ViT(num_classes=15).to(DEVICE)\n",
        "    model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        running_score = 0.0\n",
        "        running_acc = 0.0\n",
        "    \n",
        "        for inputs, labels in tqdm(loader, total=len(loader)):\n",
        "            \n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_acc += accuracy_score(labels.detach().cpu(), preds.detach().cpu())\n",
        "            running_score += f1_score(labels.detach().cpu(), preds.detach().cpu(), average='macro')\n",
        "            running_loss += loss.detach().cpu().item()\n",
        "\n",
        "        running_score /= len(loader)\n",
        "        running_loss /= len(loader)\n",
        "        running_acc /= len(loader)\n",
        "        \n",
        "        print(f'epoch = {epoch}, {running_score}, {running_acc}, {running_loss}')\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7faf0ad748484ab5a4474fa92cfb30e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c1cf35ed1254dcaa8a4b5749b6063f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch = 0, 0.026376080812191914, 0.07500000000000004, 9.123228178819021\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a847a6f8df114dbb9a4ce297c90c8252",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch = 1, 0.04848064520723254, 0.10900000000000015, 4.132748595078787\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27b5728094024f0c854c0af9bb435307",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch = 2, 0.0629956881742596, 0.13100000000000023, 3.05055091381073\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f217b85db3047539716a494c27052d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch = 3, 0.06166085225767774, 0.1203333333333336, 3.2768441208203636\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81f632b6a47d4fe2962fa8153432f639",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(\u001b[39m5\u001b[39;49m, train_loader)\n",
            "Cell \u001b[1;32mIn[25], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, loader)\u001b[0m\n\u001b[0;32m     25\u001b[0m preds \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 28\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     30\u001b[0m running_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m accuracy_score(labels\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), preds\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu())\n\u001b[0;32m     31\u001b[0m running_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m f1_score(labels\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), preds\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    363\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[0;32m    367\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(5, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f534e213486403eb5686595d60e6543",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7833333611488342\n"
          ]
        }
      ],
      "source": [
        "model = ViT(num_classes=15)\n",
        "model.load_state_dict(torch.load('./Transformer (2).pth'))\n",
        "tester = Tester(model, test_loader)\n",
        "tester.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "def report_gpu():\n",
        "   #print(torch.cuda.list_gpu_processes())\n",
        "   gc.collect()\n",
        "   torch.cuda.empty_cache()\n",
        "report_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd1043b42f44ecf814744c1775ac6e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56fa1c63a67249d690502ef5998591de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 1:  tensor([8.2502, 0.0973])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "405e017d801e46308e700f5b0ace5d9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 2:  tensor([3.3344, 0.1360])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bbd7192a1fd4b89a0b889e3bc06af5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 3:  tensor([3.3769, 0.1137])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ed901b338be4d5f81a8a23523ce87f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 4:  tensor([8.5150, 0.0907])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cddb74ce28c4b8fb65f2cd5ee1e62dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model, optim, train_loader, loss, \u001b[39m5\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "Cell \u001b[1;32mIn[22], line 29\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m (data, targets) \u001b[39min\u001b[39;00m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader)):\n\u001b[0;32m     28\u001b[0m     loss, acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(data, targets)\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward(loss)\n\u001b[0;32m     31\u001b[0m     outputs\u001b[39m.\u001b[39mappend([loss, acc])\n\u001b[0;32m     33\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(outputs)\n",
            "Cell \u001b[1;32mIn[22], line 56\u001b[0m, in \u001b[0;36mTrainer._backward\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m(\u001b[39mself\u001b[39m, loss: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32mc:\\Users\\kozlo\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = ViT(num_classes=15)\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "loss = nn.CrossEntropyLoss()\n",
        "trainer = Trainer(model, optim, train_loader, loss, 5)\n",
        "model = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QbFtayBkp-c"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZbwbK9kskc"
      },
      "source": [
        "\n",
        "1. Выбрать датасет для классификации изображений с размерностью 64x64+ \n",
        "2. Обучить ViT на таком датасете.\n",
        "3. Попробовать поменять размерности и посмотреть, что поменяется при обучении.\n",
        "\n",
        "\n",
        "Примечание:\n",
        "- Датасеты можно взять [тут](https://pytorch.org/vision/stable/datasets.html#built-in-datasets) или найти в другом месте.\n",
        "- Из за того, что ViT учится медленно, количество примеров в датасете можно ограничить до 1к-5к."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "cffdd22708c8f24895f497a03a7b67c0092c0fcb40692f19cd97059e00134830"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
