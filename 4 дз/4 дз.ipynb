{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "khe7vy_ZwLii"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Patch embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\" \n",
        "    Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, d_model=768):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.in_chans = in_chans\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embeddings = nn.Conv2d(3, self.d_model, 16, 16)\n",
        "\n",
        "    def forward(self, image):\n",
        "        b, c, h, w = image.shape\n",
        "        \n",
        "        assert h == self.img_size and w == self.img_size, f'Image size must be {self.img_size}x{self.img_size}'\n",
        "        assert c == self.in_chans, f'Image must have {self.in_chans} channels'\n",
        "        \n",
        "        patches = self.patch_embeddings(image).reshape(b, self.d_model, -1).transpose(1, 2)\n",
        "        \n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 196, 768])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn((2, 3, 224, 224))\n",
        "PatchEmbedding()(x).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Residual block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, func = None) -> None:\n",
        "        super().__init__()\n",
        "        \n",
        "        self.func = func\n",
        "        if not self.func:\n",
        "            self.func = lambda x: x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.func(x) + x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 2.,  6., 12., 20.])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.Tensor([1., 2., 3., 4.])\n",
        "ResidualBlock(lambda x: x**2)(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi Head Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4QnAW3rSc2OZ"
      },
      "outputs": [],
      "source": [
        "class MHABlock(nn.Module):\n",
        "    def __init__(self, emb_len, num_heads=8, attn_drop=0., out_drop=0.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_heads = num_heads # number of heads\n",
        "        head_emb = emb_len // num_heads # embeddings length after head\n",
        "        self.scale = head_emb ** -0.5 # scale param for decrease dispersion\n",
        "\n",
        "        self.qkv = nn.Linear(emb_len, emb_len * 3, bias=False)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        \n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(emb_len, emb_len),\n",
        "            nn.Dropout(out_drop)\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        QKV = self.qkv(x)\n",
        "        \"\"\"\n",
        "        b - batch\n",
        "        l - sequence length (number of patches)\n",
        "        n - 3 (Q K V)\n",
        "        h - num heads\n",
        "        hl - seq length after attention\n",
        "        \"\"\"\n",
        "        Q, K, V = rearrange(QKV, 'b l (n h hl) -> n b h l hl', n = 3, h = self.num_heads)\n",
        "\n",
        "        attention = F.softmax(torch.einsum('bhqo, bhko -> bhqk', Q, K) / self.scale, dim=-1)\n",
        "        attention = self.attn_drop(attention)\n",
        "        attention = attention @ V\n",
        "        attention = rearrange(attention, 'b h l hl -> b l (h hl)')\n",
        "        \n",
        "        out = self.out(attention)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 197, 768])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn((5, 197, 768))\n",
        "MHABlock(768)(x).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feed forward block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VPQts2WWdeYQ"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, in_features, mlp_ratio=4, hidden_features=None, out_features=None, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        \n",
        "        if not hidden_features:\n",
        "            hidden_features = in_features * mlp_ratio\n",
        "        if not out_features:\n",
        "            out_features = in_features\n",
        "\n",
        "        self.linears = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_features),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_rate),\n",
        "            nn.Linear(hidden_features, out_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linears(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFxxcPoMf7IW",
        "outputId": "c1f44b38-7ec4-4e93-bdb5-a4db4e116e90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "FeedForwardBlock(768)(x).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_len, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first_residual = ResidualBlock(\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(emb_len),\n",
        "                MHABlock(emb_len, num_heads),\n",
        "                nn.Dropout(drop_rate)\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        self.second_residual = ResidualBlock(\n",
        "            nn.Sequential(\n",
        "                nn.LayerNorm(emb_len),\n",
        "                FeedForwardBlock(emb_len, mlp_ratio),\n",
        "                nn.Dropout(drop_rate)\n",
        "            )\n",
        "        )           \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.first_residual(x)\n",
        "        x = self.second_residual(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aMihgfEhyql",
        "outputId": "993524e8-c7eb-4b38-802e-557e89ba8285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "block = EncoderBlock(768, 12)\n",
        "out = block(x)\n",
        "out.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer class. Stack of EncoderBlocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "b1uO18VTwLil"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, emb_len, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            EncoderBlock(emb_len, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIfp984oiBqc",
        "outputId": "704c9a49-92d6-4859-9f61-06555bf89579"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "Transformer(12, 768)(x).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Y9gyxdqQeFs6"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "                 emb_len=768, num_layers=12, num_heads=12, mlp_ratio=4, drop_rate=0.,):\n",
        "        super().__init__()\n",
        "\n",
        "        # Присвоение переменных\n",
        "        ...\n",
        "\n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        self.patch_embeddings = PatchEmbedding(img_size, patch_size, in_chans, emb_len)\n",
        "        self.cls_token = nn.Parameter(torch.randn((1, 1, emb_len)))\n",
        "        self.pos_encodings = nn.Parameter(torch.randn((self.patch_embeddings.num_patches + 1, emb_len)))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.transformer = Transformer(num_layers, emb_len, num_heads, mlp_ratio, drop_rate)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(emb_len, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        b, c, h, w = x.shape\n",
        "        \n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = self.pos_encodings + torch.cat((cls_tokens, self.patch_embeddings(x)), dim = 1)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        x = self.transformer(x)[:, 0, :].squeeze(1)\n",
        "\n",
        "        # Classifier\n",
        "        predictions = self.classifier(x)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 1000])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(10, 3, 224, 224)\n",
        "vit = ViT()\n",
        "out = vit(x)\n",
        "out.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# root_train = '../input/vegetable-image-dataset/Vegetable Images/validation'\n",
        "# root_test = '../input/vegetable-image-dataset/Vegetable Images/test'\n",
        "# transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# train_data = datasets.ImageFolder(root_train, transform)\n",
        "# test_data = datasets.ImageFolder(root_test, transform)\n",
        "\n",
        "# train_loader = DataLoader(train_data, 10, True)\n",
        "# test_loader = DataLoader(test_data, 10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn((100, 3, 224, 224))\n",
        "y = torch.randint(low=0, high=10, size=(1, 100)).squeeze(0)\n",
        "x_y = TensorDataset(x, y)\n",
        "loader = DataLoader(x_y, 10, True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        lossfunc: nn.Module,\n",
        "        epochs: int,\n",
        "        device: str = 'cuda'\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.dataloader = dataloader\n",
        "        self.lossfunc = lossfunc\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(device)\n",
        "    \n",
        "    \n",
        "    def train(self) -> nn.Module:\n",
        "        self.model.train()\n",
        "        self.model = self.model.to(self.device)\n",
        "        \n",
        "        for epoch in tqdm(range(1, self.epochs + 1), total=self.epochs):\n",
        "            outputs = []\n",
        "            for (data, targets) in tqdm(self.dataloader, total=len(self.dataloader)):\n",
        "                \n",
        "                loss, acc = self._forward(data, targets)\n",
        "                self._backward(loss)\n",
        "                \n",
        "                outputs.append([loss, acc])\n",
        "            \n",
        "            outputs = torch.Tensor(outputs)\n",
        "            print(f'Эпоха {epoch}: ',outputs.mean(dim=0))\n",
        "        \n",
        "        return self.model\n",
        "\n",
        "\n",
        "    def _forward(self, data: torch.Tensor, targets: torch.Tensor):\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        data = data.to(self.device)\n",
        "        targets = targets.to(self.device)\n",
        "        \n",
        "        logits = self.model(data)\n",
        "        \n",
        "        loss = self.lossfunc(logits, targets)\n",
        "        acc = torch.sum(logits == targets) / len(logits)\n",
        "        \n",
        "        return loss, acc\n",
        "    \n",
        "\n",
        "\n",
        "    def _backward(self, loss: torch.Tensor) -> None:\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "def report_gpu():\n",
        "   #print(torch.cuda.list_gpu_processes())\n",
        "   gc.collect()\n",
        "   torch.cuda.empty_cache()\n",
        "report_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54d670dfc07e44b984c053973d9201e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15c5e4973380441b9fd0c019962b1943",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 1:  tensor([19.1791,  0.0000])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2406be669ec4412096cba5955f240bdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 2:  tensor([15.5996,  0.0000])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99078155f4594de1abef1ad1fc24a250",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 3:  tensor([11.1823,  0.0000])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6ba2b49ed394a5486abb2d587270d4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 4:  tensor([12.4601,  0.0000])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3c77afc21a54e4b988d452b1266ae11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха 5:  tensor([9.7799, 0.0000])\n"
          ]
        }
      ],
      "source": [
        "model = ViT(num_classes=15)\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "loss = nn.CrossEntropyLoss()\n",
        "trainer = Trainer(model, optim, loader, loss, 5)\n",
        "model = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QbFtayBkp-c"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZbwbK9kskc"
      },
      "source": [
        "\n",
        "1. Выбрать датасет для классификации изображений с размерностью 64x64+ \n",
        "2. Обучить ViT на таком датасете.\n",
        "3. Попробовать поменять размерности и посмотреть, что поменяется при обучении.\n",
        "\n",
        "\n",
        "Примечание:\n",
        "- Датасеты можно взять [тут](https://pytorch.org/vision/stable/datasets.html#built-in-datasets) или найти в другом месте.\n",
        "- Из за того, что ViT учится медленно, количество примеров в датасете можно ограничить до 1к-5к."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "cffdd22708c8f24895f497a03a7b67c0092c0fcb40692f19cd97059e00134830"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
