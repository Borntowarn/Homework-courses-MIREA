{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import hydra\n",
    "import enchant\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import straug.blur as blur\n",
    "import straug.warp as warp\n",
    "import straug.noise as noise\n",
    "import straug.camera as camera\n",
    "import straug.process as process\n",
    "import straug.geometry as geometry\n",
    "\n",
    "from tkinter import *\n",
    "from torch import Tensor\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Union, Optional\n",
    "from IPython.display import display\n",
    "from hydra import initialize, compose\n",
    "from PIL import Image, ImageTk, ImageDraw\n",
    "from pytorch_lightning import seed_everything\n",
    "from ctc_decoder import best_path, beam_search\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchmetrics import CharErrorRate, WordErrorRate\n",
    "from omegaconf import OmegaConf, DictConfig, open_dict\n",
    "\n",
    "seed_everything(0, True)\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class for creating custom image2label dataset from folder\n",
    "\n",
    "    Args:\n",
    "        Dataset (Dataset): Standart torch class for custom datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        label_dir: str,\n",
    "        transforms: transforms.Compose = None) -> None:\n",
    "        \n",
    "        super(HWTDataset, self).__init__()\n",
    "        \n",
    "        # Loading labling file\n",
    "        name_label = pd.read_csv(label_dir, delimiter='\\t', names = ['Image name', 'Label'])\n",
    "        name_label['Image name'] = name_label['Image name'].apply(lambda x: os.path.join(root_dir, x))\n",
    "        self.data = name_label.to_dict('split')['data']\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, str]:\n",
    "        \n",
    "        path, label = self.data[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolCoder:\n",
    "    \"\"\"\n",
    "    Class needs to encode initial phrases to Tensor\n",
    "    and decode predicted labels to phrases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alphabet) -> None:\n",
    "        \n",
    "        self.alphabet = ''.join(sorted(alphabet))\n",
    "        self.sym2class, self.class2sym = {'' : 0}, {0 : ''}\n",
    "        \n",
    "        for num, alpha in enumerate(self.alphabet):\n",
    "            self.sym2class[alpha] = num + 1\n",
    "            self.class2sym[num + 1] = alpha\n",
    "    \n",
    "    \n",
    "    def encode(self, text) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        This method encode initial phrases to Tensor\n",
    "\n",
    "        Args:\n",
    "            text (list): Initial phrases for encode\n",
    "\n",
    "        Returns:\n",
    "            tuple: First value is a tensor of phrases labels, second is lengths of phrases\n",
    "        \"\"\"\n",
    "        \n",
    "        length = []\n",
    "        result = []\n",
    "        \n",
    "        for word in text:\n",
    "            length.append(len(word))\n",
    "            for alpha in word:\n",
    "                if alpha in self.alphabet: \n",
    "                    result.append(self.sym2class[alpha])\n",
    "                else: result.append(0)\n",
    "        \n",
    "        return (torch.tensor(result, dtype=torch.int64), torch.tensor(length, dtype=torch.int64))\n",
    "    \n",
    "    \n",
    "    def decode(self, text, length) -> Union[str, list]:\n",
    "        \"\"\"\n",
    "        This method used for decoding prediction labels to text\n",
    "\n",
    "        Args:\n",
    "            text (Tensor): predicted labels of symbols\n",
    "            length (Tensor): lengths of prediction phrases\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: list type returns when use for batch, for single word returns str\n",
    "        \"\"\"\n",
    "        \n",
    "        #For single word\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            word = ''\n",
    "            \n",
    "            for i in range(length):\n",
    "                if text[i] != 0 and not (i > 0 and text[i - 1] == text[i]):\n",
    "                    word  += self.class2sym[text[i].item()]\n",
    "            return word\n",
    "        \n",
    "        #For batch\n",
    "        else:\n",
    "            words = []\n",
    "            index = 0\n",
    "            \n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                words.append(self.decode(text[index:index + l], torch.IntTensor([l])))\n",
    "                index += l\n",
    "            return words\n",
    "    \n",
    "    \n",
    "    def beam_decode(self, logits, batch_num, beam_width = 5) -> list[str]:\n",
    "        predictions = []\n",
    "        for i in range(batch_num):\n",
    "            word = torch.nn.functional.softmax(logits[:, i, :], dim = 1)\n",
    "            word = torch.hstack((word, word[:, 0].unsqueeze(1)))[:, 1:].cpu().numpy()\n",
    "            res = beam_search(word, self.alphabet, beam_width) # Over 10 is too slow\n",
    "            predictions.append(res)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.class2sym)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms(transforms.Compose):\n",
    "    \n",
    "    def __init__(self, args) -> None:\n",
    "        \n",
    "        self.transforms = []\n",
    "        \n",
    "        for key, value in args.items():\n",
    "            value = OmegaConf.to_object(value)\n",
    "            self.transforms.append(\n",
    "                transforms.RandomApply([\n",
    "                    getattr(transforms, key)(**value['params'])], # Transform\n",
    "                    value['prob']) # Probability of apply\n",
    "                )\n",
    "        self.transforms.append(transforms.ToTensor())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class use for seq 2 seq prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_len,\n",
    "                 out_len,\n",
    "                 n_classes : int = None,\n",
    "                 rnn_type : str = 'RNN',\n",
    "                 bidirectional : bool = True,\n",
    "                 batch_first : bool = True) -> None:\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.linear = None\n",
    "        \n",
    "        self.rnn = getattr(nn, rnn_type)(in_len, out_len, \n",
    "                          bidirectional = bidirectional, \n",
    "                          batch_first = batch_first)\n",
    "        \n",
    "        if self.n_classes:\n",
    "            self.linear = nn.Linear(out_len * [1, 2][bidirectional], n_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, data) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        N = batch size\n",
    "        L = sequence length\n",
    "        D = 2 if bidirectional=True otherwise 1\n",
    "        H_in = input_size\n",
    "        H_out = hidden_size\n",
    "        \"\"\"\n",
    "        \n",
    "        N, L, H_in = data.shape\n",
    "        \n",
    "        data, _ = self.rnn(data) # [N, L, D * n_hidden]\n",
    "        if self.linear:\n",
    "            data = data.reshape(N * L, -1)\n",
    "            data = self.linear(data)\n",
    "            data = data.reshape(N, L, -1)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    This class use for slicing initial image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels : int,\n",
    "                 img_shape : tuple,\n",
    "                 len_alphabet : int,\n",
    "                 num_layers : int = 5,\n",
    "                 increase_channels_layers : list = [1, 2, 4],\n",
    "                 modules_seq : str = 'CAMB',\n",
    "                 modules_freq : list = [1, 1, 1, 1],\n",
    "                 conv_kernel_size : Union[int, tuple] = 3,\n",
    "                 conv_stride : Union[int, tuple] = 1,\n",
    "                 conv_padding : Union[int, tuple] = 1,\n",
    "                 pool_kernel_size : Union[int, tuple] = 2,\n",
    "                 pool_stride : Union[int, tuple] = 2,\n",
    "                 pool_padding : Union[int, tuple] = 0,\n",
    "                 activation : str = 'ReLU',\n",
    "                 rnn_type : str = 'RNN'\n",
    "                 ) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        out_channels = 64\n",
    "        \n",
    "        # Frequence of append every module. For example:\n",
    "        # if freq for MaxPool is 2 then MaxPool will be appended every 2 layer\n",
    "        frequency = dict(zip(modules_seq, modules_freq)) \n",
    "        \n",
    "        # For every layer create sequence of modules\n",
    "        for layer in range(1, num_layers + 1):\n",
    "            for module in modules_seq:\n",
    "                if layer % frequency[module]: # Check freq of module\n",
    "                    continue\n",
    "                if module == 'C':\n",
    "                    self.layers.append(nn.Conv2d(in_channels, out_channels, conv_kernel_size,\n",
    "                                                conv_stride, conv_padding))\n",
    "                    in_channels = out_channels\n",
    "                    img_shape = self.conv_output_shape(img_shape, conv_kernel_size, conv_stride, conv_padding)\n",
    "                elif module == 'A':\n",
    "                    self.layers.append(getattr(nn, activation)())\n",
    "                elif module == 'M':\n",
    "                    self.layers.append(nn.MaxPool2d(pool_kernel_size, pool_stride, pool_padding))\n",
    "                    img_shape = self.conv_output_shape(img_shape, pool_kernel_size, pool_stride, pool_padding)\n",
    "                elif module == 'B':\n",
    "                    self.layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            if layer in increase_channels_layers:\n",
    "                out_channels *= 2\n",
    "        \n",
    "        self.rnn_layers = nn.Sequential(\n",
    "            RNN(img_shape[0]*512, 512, rnn_type=rnn_type, bidirectional=True, batch_first=True),\n",
    "            RNN(1024, 128, len_alphabet + 1, rnn_type=rnn_type, bidirectional=True, batch_first=True)\n",
    "        )\n",
    "        \n",
    "        print(f'Shape after convs layers: {img_shape}')\n",
    "    \n",
    "    \n",
    "    def conv_output_shape(self,\n",
    "                          h_w : tuple,\n",
    "                          kernel_size : Union[int, tuple] = 1,\n",
    "                          stride : Union[int, tuple] = 1,\n",
    "                          pad : Union[int, tuple] = 0,\n",
    "                          dilation : Union[int, tuple] = 1\n",
    "                          ) -> tuple:\n",
    "        \"\"\"\n",
    "        This method calculate out height and width of img\n",
    "\n",
    "        Args:\n",
    "            h_w (tuple): Input shape.\n",
    "            kernel_size (Union[int, tuple], optional): Defaults to 1.\n",
    "            stride (Union[int, tuple], optional): Defaults to 1.\n",
    "            pad (Union[int, tuple], optional): Defaults to 0.\n",
    "            dilation (Union[int, tuple], optional): _description_. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Output shape\n",
    "        \"\"\"\n",
    "        from math import floor\n",
    "        \n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(pad, int):\n",
    "            pad = (pad, pad)\n",
    "            \n",
    "        h = floor(((h_w[0] + (2 * pad[0]) - (dilation * (kernel_size[0] - 1)) - 1) / stride[0]) + 1)\n",
    "        w = floor(((h_w[1] + (2 * pad[0]) - (dilation * (kernel_size[1] - 1)) - 1) / stride[1]) + 1)\n",
    "        return (h, w)\n",
    "    \n",
    "    \n",
    "    def forward(self, data):\n",
    "        \n",
    "        for module in self.layers:\n",
    "            data = module(data)\n",
    "        \n",
    "        bs, c, h, w = data.shape\n",
    "        \n",
    "        data = data.permute(0, 3, 1, 2).reshape(bs, w, c * h) # Out - bs, w, h * c (N, L, H)\n",
    "        data = self.rnn_layers(data) # Out - N, L, len_alphabet\n",
    "        data = data.permute(1, 0, 2) # Out - L, N, len_alphabet\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(data, 2)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 optimizer,\n",
    "                 dataloader,\n",
    "                 lossfunc,\n",
    "                 coder,\n",
    "                 epochs,\n",
    "                 model_name,\n",
    "                 train_alphabet,\n",
    "                 scheduler = None,\n",
    "                 logging : bool = False,\n",
    "                 device : str = 'cuda'\n",
    "                 ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.dataloader = dataloader\n",
    "        self.lossfunc = lossfunc\n",
    "        self.coder = coder\n",
    "        self.epochs = epochs\n",
    "        self.model_name = model_name\n",
    "        self.LOGGING = logging\n",
    "        self.DEVICE = device\n",
    "        self.train_alphabet = train_alphabet\n",
    "\n",
    "\n",
    "    def print_epoch_data(self,\n",
    "                         epoch: int,\n",
    "                         mean_loss: float,\n",
    "                         char_error: float,\n",
    "                         word_error: float,\n",
    "                         zero_out_losses: float\n",
    "                         ) -> None:\n",
    "        \"\"\"\n",
    "        This method printing epoch statistics\n",
    "        \"\"\"\n",
    "        \n",
    "        print(tabulate(\n",
    "            [['epoch', 'mean loss', 'mean cer', 'mean wer', 'zero loss warnings'],\n",
    "             [epoch, round(mean_loss, 4), round(char_error, 4),\n",
    "              round(word_error, 4), zero_out_losses]],\n",
    "            headers='firstrow',\n",
    "            tablefmt='fancy_grid'))\n",
    "\n",
    "\n",
    "    def save_model(self, mean_loss: float, char_error: float) -> None:\n",
    "        torch.save(self.model.state_dict(),\n",
    "                    f'./{self.model_name} \\\n",
    "                    _L-{round(mean_loss, 4)} \\\n",
    "                    _CER-{round(char_error, 4)}.pth')\n",
    "\n",
    "\n",
    "    def log(self, mean_loss: float, char_error: float, word_error: float) -> None:\n",
    "        wandb.log({'loss': mean_loss,\n",
    "                    'CER': char_error,\n",
    "                    'WER': word_error,\n",
    "                    'Learn Rate': \n",
    "                        self.scheduler.get_last_lr()[-1] \n",
    "                        if self.scheduler \n",
    "                        else self.optimizer.param_groups[0]['lr']})\n",
    "    \n",
    "    \n",
    "    def print_save_stat(self, outputs: list, epoch: int, zero_out: int) -> None:\n",
    "        \n",
    "        assert len(outputs) != 0, 'Error: bad loss'\n",
    "            \n",
    "        output = torch.Tensor(outputs)\n",
    "        mean_loss = output[:, 0].mean().item()\n",
    "        char_error = output[:, 1].mean().item()\n",
    "        word_error = output[:, 2].mean().item()\n",
    "        \n",
    "        self.print_epoch_data(epoch, mean_loss, char_error, word_error, zero_out)\n",
    "        \n",
    "        if self.LOGGING:\n",
    "            self.log(mean_loss, char_error, word_error)\n",
    "        \n",
    "        if mean_loss < 0.1 or not (epoch + 1) % 5:\n",
    "            self.save_model(mean_loss, char_error)\n",
    "    \n",
    "    def forward(self, data: Tensor, targets: Tensor) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        classes, lengths = self.coder.encode(targets)\n",
    "        data = data.to(self.DEVICE)\n",
    "        classes = classes.to(self.DEVICE)\n",
    "        \n",
    "        logits = self.model(data)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, N, C = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(N)]).to(self.DEVICE)\n",
    "        classes = classes.view(-1).contiguous()\n",
    "        loss = self.lossfunc(logits, classes, pred_sizes, lengths)\n",
    "    \n",
    "    \n",
    "    def prediction(self, logits: Tensor, pred_sizes: Tensor) -> list[str]:\n",
    "        probs, preds = logits.max(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = self.coder.decode(preds.data, pred_sizes.data)\n",
    "        return sim_preds\n",
    "\n",
    "\n",
    "    def backward(self, loss: Tensor, sim_preds: list, targets: list) -> tuple[float, float]:\n",
    "        CER = CharErrorRate()\n",
    "        WER = WordErrorRate()\n",
    "        cer = CER(sim_preds, targets)\n",
    "        wer = WER(sim_preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        return cer, wer\n",
    "    \n",
    "    \n",
    "    def statistics(self, loss: Tensor, cer: float, wer: float) -> list[float, float, float]:\n",
    "        return [abs(loss.item()), cer, wer]\n",
    "    \n",
    "    \n",
    "    def train(self) -> Model:\n",
    "        self.model.train()\n",
    "        \n",
    "        if self.LOGGING:\n",
    "            wandb.watch(self.model, self.lossfunc, log='all', log_freq=100)\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs), total=self.epochs):\n",
    "            zero_out_losses = 0\n",
    "            outputs = []\n",
    "            for (data, targets) in tqdm(self.dataloader, total=len(self.dataloader)):\n",
    "                \n",
    "                loss, logits, pred_sizes = self.forward(data, targets)\n",
    "                sim_preds = self.prediction(logits, pred_sizes)\n",
    "                cer, wer = self.backward(loss, sim_preds, targets)\n",
    "                outputs.append(self.statistics(loss, cer, wer))\n",
    "            \n",
    "            self.print_save_stat(outputs, epoch, zero_out_losses)\n",
    "        \n",
    "        return self.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Class for evaluate CER, WER of model and\n",
    "    count stat about symbols errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 loader,\n",
    "                 coder,\n",
    "                 device : str = 'cuda'\n",
    "                 ) -> None:\n",
    "        self.model = model.eval()\n",
    "        self.CER = CharErrorRate()\n",
    "        self.WER = WordErrorRate()\n",
    "        self.coder = coder\n",
    "        self.loader = loader\n",
    "        self.device = device\n",
    "        self.avg_matches = 0\n",
    "        \n",
    "        self.original_pred = []\n",
    "        self.original_labels = []\n",
    "        \n",
    "        self.symbol_err = {}\n",
    "        self.length_word_CER = {}\n",
    "        \n",
    "    \n",
    "    def suggest(self, words: list, dictionary: enchant.Dict) -> str:\n",
    "        result = ''\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isalpha():\n",
    "                \n",
    "                # If word is in dict probably it's without errors\n",
    "                cer_suggest = dict()\n",
    "                if dictionary.check(word):\n",
    "                    result += word + ' '\n",
    "                    continue\n",
    "                \n",
    "                # Else dict can suggest what word we need\n",
    "                suggestions = set(dictionary.suggest(word))\n",
    "\n",
    "                # For every suggestion finding CER\n",
    "                for suggest in suggestions:\n",
    "                    if ' ' not in suggest:\n",
    "                        cer = self.CER(suggest, word)\n",
    "                        if cer.item() / len(word) < 0.05:\n",
    "                            cer_suggest[cer] = suggest\n",
    "                \n",
    "                # Get the nearest word\n",
    "                if len(cer_suggest.keys()) > 0: \n",
    "                    result += cer_suggest[min(cer_suggest.keys())] + ' '\n",
    "                # Or take original word if there's no suggestions\n",
    "                else:\n",
    "                    result += word + ' '\n",
    "                    \n",
    "        return result[:-1]\n",
    "\n",
    "    \n",
    "    def word_correction(self, predictions: list[str]) -> list:\n",
    "        correct_predictions = []\n",
    "        dictionary = enchant.Dict(\"ru_RU\")\n",
    "        \n",
    "        # Every pred phrase is splitted by words\n",
    "        # then every word is checked with external dict\n",
    "        for phrase in tqdm(predictions, total=len(predictions)):\n",
    "            words = phrase.split()\n",
    "            result = self.suggest(words, dictionary)\n",
    "            correct_predictions.append(result)\n",
    "            \n",
    "        return correct_predictions\n",
    "    \n",
    "    \n",
    "    def count_errors(self) -> None:\n",
    "        \n",
    "        if not len(self.original_pred):\n",
    "            self.evaluate()\n",
    "        \n",
    "        for pred, true in zip(self.original_pred, self.original_labels):\n",
    "            \n",
    "            # Add CERs for all pairs (pred, true) to collect\n",
    "            # the errors dependence on the length\n",
    "            if len(true) in self.length_word_CER.keys():\n",
    "                self.length_word_CER[len(true)].append(self.CER(pred, true))\n",
    "            else: \n",
    "                self.length_word_CER[len(true)] = [self.CER(pred, true)]\n",
    "            \n",
    "            # Collect pairs of mismatched symbols\n",
    "            if len(true) == len(pred) and true != pred:\n",
    "                for i, j in zip(pred, true):\n",
    "                    if i != j: \n",
    "                        if i in self.symbol_err.keys(): \n",
    "                            if j in self.symbol_err[i].keys():\n",
    "                                self.symbol_err[i][j] += 1\n",
    "                                self.avg_matches += 1\n",
    "                            else: self.symbol_err[i][j] = 1\n",
    "                        else: self.symbol_err[i] = {j : 1}\n",
    "    \n",
    "    \n",
    "    def errors_sym_stat(self) -> tuple[dict[str, dict], dict[str, float]]:\n",
    "    \n",
    "        if not len(self.length_word_CER):\n",
    "            self.count_errors()\n",
    "            self.avg_matches /= len(self.symbol_err.keys())\n",
    "        \n",
    "            # For every error symbol leavy only with number of errors >= self.avg_matches\n",
    "            for pred_sym in self.symbol_err.keys():\n",
    "                self.symbol_err[pred_sym] = dict(filter(\n",
    "                    lambda elem: elem[1] >= self.avg_matches, \n",
    "                    self.symbol_err[pred_sym].items()))\n",
    "                \n",
    "            # Delete empty dictionaries in final stat\n",
    "            self.symbol_err = dict(filter(\n",
    "                lambda elem: len(elem[1]) > 0,\n",
    "                self.symbol_err.items()))\n",
    "\n",
    "            # Count mean value for CER based on length\n",
    "            self.length_word_CER = {\n",
    "                key : torch.Tensor(self.length_word_CER[key]).mean().item()\n",
    "                for key in self.length_word_CER.keys()\n",
    "                }\n",
    "        \n",
    "        return self.symbol_err.copy(), self.length_word_CER.copy()\n",
    "    \n",
    "    \n",
    "    def forward(self, beam_width: int) -> tuple[list[str], list[str]]:\n",
    "        \n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        for iteration, batch in enumerate(tqdm(self.loader)):\n",
    "            data, targets = batch[0].to(self.device), batch[1]\n",
    "            labels.extend(targets)\n",
    "            \n",
    "            logits = self.model(data).contiguous().detach()\n",
    "            T, B, H = logits.size()\n",
    "            pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "            probs, pos = logits.max(2)\n",
    "            pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "            \n",
    "            if beam_width:\n",
    "                sim_preds = self.coder.beam_decode(logits, B, beam_width)\n",
    "            else:\n",
    "                sim_preds = self.coder.decode(pos.data, pred_sizes.data)\n",
    "                \n",
    "            predictions.extend(sim_preds)\n",
    "        \n",
    "        if not len(self.original_labels):\n",
    "            self.original_pred = predictions\n",
    "            self.original_labels = labels\n",
    "            \n",
    "        return predictions, labels\n",
    "\n",
    "    def evaluate(self, beam_width: int = 0, correcting: bool = False) -> tuple:\n",
    "        \n",
    "        predictions, labels = self.forward(beam_width)\n",
    "        \n",
    "        # Correct predictions if wants\n",
    "        if correcting: \n",
    "            predictions = self.word_correction(predictions)\n",
    "        \n",
    "        # Count CER, WER\n",
    "        char_error = self.CER(predictions, labels)\n",
    "        word_error = self.WER(predictions, labels)\n",
    "        \n",
    "        return char_error, word_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recognizer:\n",
    "    \"\"\"\n",
    "    This class can recognize phrase from painted or given picture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 coder,\n",
    "                 transform: Transforms,\n",
    "                 device = 'cuda') -> None:\n",
    "        self.model = model.eval()\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.coder = coder\n",
    "    \n",
    "    \n",
    "    def forward(self, img: Image, beam_width: int) -> str:\n",
    "        \"\"\"\n",
    "        This method implements forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            img (Image): Given image to recognize.\n",
    "            If Image then transforms must be True. Else it's assumed transforms have already been applied\n",
    "\n",
    "        Returns:\n",
    "            str: Recognized phrase\n",
    "        \"\"\"\n",
    "        img = self.transform(img).unsqueeze(0)\n",
    "\n",
    "        logits = self.model(img.to(self.device))\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T])\n",
    "        probs, pos = logits.max(2)\n",
    "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "        if beam_width:\n",
    "            prediction = self.coder.beam_decode(logits, B, beam_width)\n",
    "        else:\n",
    "            prediction = self.coder.decode(pos.data, pred_sizes.data)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def paint(self) -> None:\n",
    "        \"\"\"\n",
    "        This method creates a window to paint a phrase\n",
    "        \"\"\"\n",
    "        width = 1000  # canvas width\n",
    "        height = 400 # canvas height\n",
    "        center = height//2\n",
    "        white = (255, 255, 255) # canvas back\n",
    "        \n",
    "        self.master = Tk()\n",
    "\n",
    "        # Create a tkinter canvas to draw on\n",
    "        self.canvas = Canvas(self.master, width=width, height=height, bg='white')\n",
    "        self.canvas.pack()\n",
    "\n",
    "        # Create an empty PIL image and draw object to draw on\n",
    "        self.img = PIL.Image.new(\"RGB\", (width, height), white)\n",
    "        self.draw = ImageDraw.Draw(self.img)\n",
    "        self.canvas.pack(expand=YES, fill=BOTH)\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_img)\n",
    "\n",
    "        # Button to recognize img and close pint window\n",
    "        button=Button(text=\"Recognize\",command=self.master.destroy)\n",
    "        button.pack()\n",
    "        \n",
    "        self.master.mainloop()\n",
    "    \n",
    "\n",
    "    def draw_img(self, event) -> None:\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        self.canvas.create_oval(x1, y1, x2, y2, fill=\"black\",width=5)\n",
    "        self.draw.line([x1, y1, x2, y2],fill=\"black\",width=5)\n",
    "\n",
    "\n",
    "    def recognize_from_painted(self, beam_width: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        This method allows you to paint phrase to recognize\n",
    "\n",
    "        Returns:\n",
    "            str: Recognized phrase\n",
    "        \"\"\"\n",
    "        self.paint()\n",
    "        prediction = self.forward(self.img, beam_width)\n",
    "        return prediction\n",
    "\n",
    "    \n",
    "    def recognize_from_file(self, path, beam_width: int = 0, correcting = False) -> str:\n",
    "        \"\"\"\n",
    "        This method loads img from given then recognize it\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to img\n",
    "            correcting (bool, optional): If you want to correct predicted word with dictionary. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str: Recognized phrase\n",
    "        \"\"\"\n",
    "        img = PIL.Image.open(path)\n",
    "        prediction = self.forward(img, beam_width)\n",
    "        \n",
    "        if correcting: \n",
    "            evaluator = Evaluator(self.model, None, self.coder)\n",
    "            prediction = evaluator.word_correction([prediction])[0]\n",
    "            \n",
    "        return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'config'\n",
    "CONFIG_NAME = 'config 2'\n",
    "\n",
    "with initialize(version_base=None, config_path=CONFIG_PATH):\n",
    "    cfg = compose(CONFIG_NAME)\n",
    "    cfg = compose(CONFIG_NAME, [f'+transforms={cfg.transforms}',\n",
    "                                f'+model={cfg.model}',\n",
    "                                f'+scheduler={cfg.scheduler}',\n",
    "                                f'+optim={cfg.optim}'])\n",
    "    if cfg.transforms.params.Grayscale.prob > 0:\n",
    "        in_channels = cfg.transforms.params.Grayscale.params.num_output_channels\n",
    "    else:\n",
    "        in_channels = 3\n",
    "    img_shape = cfg.transforms.params.Resize.params.size\n",
    "    with open_dict(cfg):\n",
    "        cfg.model.params.in_channels = in_channels\n",
    "        cfg.model.params.img_shape = img_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.id_resume and cfg.logging:\n",
    "    wandb.init(\n",
    "        id=cfg.id_resume,\n",
    "        project=\"Handwritten text recognition\",\n",
    "        resume='must'\n",
    "    )\n",
    "elif cfg.logging:\n",
    "    wandb.init(\n",
    "        project=\"Handwritten text recognition\",\n",
    "        name = f\"{cfg.model.name}_{cfg.transforms.name}_{cfg.optim.name}_{cfg.scheduler.name}\",\n",
    "        config={\n",
    "            'Model': cfg.model.name,\n",
    "            'Transform': cfg.transforms.name,\n",
    "            'Optimizer': cfg.optim.name,\n",
    "            'Scheduler': cfg.scheduler.name if cfg.scheduler else 'None',\n",
    "            'architecture': 'RCNN',\n",
    "            'dataset': 'Handwritten Cyrillic dataset' if cfg.dataset == 'old_' else 'Custom dataset',\n",
    "            'epochs': cfg.epochs,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transforms(cfg.transforms.params)\n",
    "\n",
    "train_data = HWTDataset(cfg.train.dir,\n",
    "                        cfg.train.labels,\n",
    "                        transform)\n",
    "test_data = HWTDataset(cfg.test.dir,\n",
    "                       cfg.test.labels,\n",
    "                       transform)\n",
    "\n",
    "train_df = pd.read_csv(cfg.train.labels, delimiter='\\t', names = ['Image name', 'Label'])\n",
    "train_alphabet = set(train_df['Label'].to_string()) - set('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after convs layers: (8, 32)\n"
     ]
    }
   ],
   "source": [
    "model = Model(**cfg.model.params)\n",
    "model = model.to(cfg.device)\n",
    "\n",
    "coder = SymbolCoder(train_alphabet)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, cfg.train_batch, True)\n",
    "test_dataloader = DataLoader(test_data, cfg.test_batch, True)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "optimizer = getattr(torch.optim, cfg.optim.optim)(model.parameters(), **cfg.optim.params)\n",
    "\n",
    "if cfg.scheduler:\n",
    "    if cfg.scheduler.scheduler == 'OneCycleLR':\n",
    "        cfg.scheduler.params.total_steps *= len(train_dataloader)\n",
    "    if cfg.scheduler.scheduler == 'StepLR':\n",
    "        cfg.scheduler.params.step_size *= len(train_dataloader)\n",
    "    scheduler = getattr(torch.optim.lr_scheduler,\n",
    "                        cfg.scheduler.scheduler)(optimizer,\n",
    "                                                    **cfg.scheduler.params)\n",
    "else: scheduler = None\n",
    "\n",
    "trainer = Trainer(model, optimizer, scheduler, train_dataloader, ctc_loss, coder,\n",
    "                  cfg.epochs, f'{cfg.model.name}_{cfg.transforms.name}_{cfg.optim.name}_{cfg.scheduler.name}',\n",
    "                  train_alphabet, cfg.logging, cfg.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обычная оценка тестовых данных, без коррекции слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model, test_dataloader, coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa80004daa0a4523876bf2f51aff56a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0660), tensor(0.2530))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка с beam search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение значения beam_width > 5 практически никак не влияло на результат, поэтому оставил 5, так получилось быстрее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b0eec659b0446dafa1240e1bcae91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0950), tensor(0.3674))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие символы чаще путает наша модель, а также ошибки в зависимости от длины последовательности. На основе этой статистики возможно реализовать замену вероятных ошибочных символов и искать ближайшее совпадение в словаре (с учетом того, что в нашем тексте нет требований к сохранению четкой орфографии писавшего человека). А на основе соотношения длины к ошибке можно изменить модель - увеличить или уменьшить входящую в RNN последовательность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'и': {'н': 52, 'И': 58},\n",
       " 'н': {'Н': 61, 'к': 63},\n",
       " 'о': {'а': 137, 'О': 50},\n",
       " 'ы': {'Ы': 90},\n",
       " 'к': {'К': 70},\n",
       " 'т': {'Т': 49},\n",
       " 'с': {'С': 92},\n",
       " 'К': {'к': 60},\n",
       " 'С': {'с': 50},\n",
       " 'А': {'а': 76},\n",
       " 'р': {'Р': 49},\n",
       " 'а': {'А': 135, 'о': 69},\n",
       " 'Т': {'т': 49}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat = evaluator.errors_sym_stat()\n",
    "stat[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С проверкой орфографии слов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корректировка предсказаний по словарю. Результат только ухудшился. Нужно или менять словарь, или писать корректировку на основе ошибочных символов (выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ded76e1c37346369231a438fc4cbb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1097678190da4ccaaed324911cc5e7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3060), tensor(0.4560))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(correcting=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на CER в зависимости от длины последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2438a4da9d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(dict(sorted(stat[1].items())).keys())\n",
    "y = dict(sorted(stat[1].items())).values()\n",
    "\n",
    "plt.ylabel('CER')\n",
    "plt.xlabel('Длина последовательности')\n",
    "plt.xticks(x)\n",
    "plt.scatter(x, y, color = 'red', label = 'CER последовательности')\n",
    "plt.plot(x, [0.095 for i in x], color = 'blue', label = 'CER на всей выборке')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рисование слова и его предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'конфискация'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer = Recognizer(model, coder, transform, 'cuda')\n",
    "recognizer.recognize_from_painted()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cffdd22708c8f24895f497a03a7b67c0092c0fcb40692f19cd97059e00134830"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
