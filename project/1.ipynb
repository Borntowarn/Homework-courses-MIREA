{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import math\n",
    "import wandb\n",
    "import torch\n",
    "import hydra\n",
    "import enchant\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import straug.blur as blur\n",
    "import straug.warp as warp\n",
    "import straug.noise as noise\n",
    "import straug.camera as camera\n",
    "import straug.process as process\n",
    "import straug.geometry as geometry\n",
    "\n",
    "from tkinter import *\n",
    "from torch import Tensor\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Union, Optional\n",
    "from IPython.display import display\n",
    "from hydra import initialize, compose\n",
    "from PIL import Image, ImageTk, ImageDraw\n",
    "from pytorch_lightning import seed_everything\n",
    "from ctc_decoder import best_path, beam_search\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchmetrics import CharErrorRate, WordErrorRate\n",
    "from omegaconf import OmegaConf, DictConfig, open_dict\n",
    "\n",
    "seed_everything(0, True)\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class for creating custom image2label dataset from folder\n",
    "\n",
    "    Args:\n",
    "        Dataset (Dataset): Standart torch class for custom datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        label_dir: str,\n",
    "        transforms: transforms.Compose = None) -> None:\n",
    "        \n",
    "        super(HWTDataset, self).__init__()\n",
    "        \n",
    "        # Loading labling file\n",
    "        name_label = pd.read_csv(label_dir, delimiter='\\t', names = ['Image name', 'Label'])\n",
    "        name_label['Image name'] = name_label['Image name'].apply(lambda x: os.path.join(root_dir, x))\n",
    "        self.data = name_label.to_dict('split')['data']\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, str]:\n",
    "        \n",
    "        path, label = self.data[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolCoder:\n",
    "    \"\"\"\n",
    "    Class needs to encode initial phrases to Tensor\n",
    "    and decode predicted labels to phrases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alphabet) -> None:\n",
    "        \n",
    "        self.alphabet = ''.join(sorted(alphabet))\n",
    "        self.sym2class, self.class2sym = {'' : 0}, {0 : ''}\n",
    "        \n",
    "        for num, alpha in enumerate(self.alphabet):\n",
    "            self.sym2class[alpha] = num + 1\n",
    "            self.class2sym[num + 1] = alpha\n",
    "    \n",
    "    \n",
    "    def encode(self, text) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        This method encode initial phrases to Tensor\n",
    "\n",
    "        Args:\n",
    "            text (list): Initial phrases for encode\n",
    "\n",
    "        Returns:\n",
    "            tuple: First value is a tensor of phrases labels, second is lengths of phrases\n",
    "        \"\"\"\n",
    "        \n",
    "        length = []\n",
    "        result = []\n",
    "        \n",
    "        for word in text:\n",
    "            length.append(len(word))\n",
    "            for alpha in word:\n",
    "                if alpha in self.alphabet: \n",
    "                    result.append(self.sym2class[alpha])\n",
    "                else: result.append(0)\n",
    "        \n",
    "        return (torch.tensor(result, dtype=torch.int64), torch.tensor(length, dtype=torch.int64))\n",
    "    \n",
    "    \n",
    "    def decode(self, text, length) -> Union[str, list]:\n",
    "        \"\"\"\n",
    "        This method used for decoding prediction labels to text\n",
    "\n",
    "        Args:\n",
    "            text (Tensor): predicted labels of symbols\n",
    "            length (Tensor): lengths of prediction phrases\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: list type returns when use for batch, for single word returns str\n",
    "        \"\"\"\n",
    "        \n",
    "        #For single word\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            word = ''\n",
    "            \n",
    "            for i in range(length):\n",
    "                if text[i] != 0 and not (i > 0 and text[i - 1] == text[i]):\n",
    "                    word  += self.class2sym[text[i].item()]\n",
    "            return word\n",
    "        \n",
    "        #For batch\n",
    "        else:\n",
    "            words = []\n",
    "            index = 0\n",
    "            \n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                words.append(self.decode(text[index:index + l], torch.IntTensor([l])))\n",
    "                index += l\n",
    "            return words\n",
    "    \n",
    "    \n",
    "    def beam_decode(self, logits, batch_num, beam_width = 5) -> list[str]:\n",
    "        predictions = []\n",
    "        for i in range(batch_num):\n",
    "            word = torch.nn.functional.softmax(logits[:, i, :], dim = 1)\n",
    "            word = torch.hstack((word, word[:, 0].unsqueeze(1)))[:, 1:].cpu().numpy()\n",
    "            res = beam_search(word, self.alphabet, beam_width) # Over 10 is too slow\n",
    "            predictions.append(res)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.class2sym)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms(transforms.Compose):\n",
    "    \n",
    "    def __init__(self, args) -> None:\n",
    "        \n",
    "        self.transforms = []\n",
    "        \n",
    "        for key, value in args.items():\n",
    "            value = OmegaConf.to_object(value)\n",
    "            self.transforms.append(\n",
    "                transforms.RandomApply([\n",
    "                    getattr(transforms, key)(**value['params'])], # Transform\n",
    "                    value['prob']) # Probability of apply\n",
    "                )\n",
    "        self.transforms.append(transforms.ToTensor())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This class use for seq 2 seq prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_len,\n",
    "                 out_len,\n",
    "                 n_classes : int = None,\n",
    "                 rnn_type : str = 'RNN',\n",
    "                 bidirectional : bool = True,\n",
    "                 batch_first : bool = True) -> None:\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.linear = None\n",
    "        \n",
    "        self.rnn = getattr(nn, rnn_type)(in_len, out_len, \n",
    "                          bidirectional = bidirectional, \n",
    "                          batch_first = batch_first)\n",
    "        \n",
    "        if self.n_classes:\n",
    "            self.linear = nn.Linear(out_len * [1, 2][bidirectional], n_classes)\n",
    "\n",
    "    \n",
    "    def forward(self, data) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        N = batch size\n",
    "        L = sequence length\n",
    "        D = 2 if bidirectional=True otherwise 1\n",
    "        H_in = input_size\n",
    "        H_out = hidden_size\n",
    "        \"\"\"\n",
    "        \n",
    "        N, L, H_in = data.shape\n",
    "        \n",
    "        data, _ = self.rnn(data) # [N, L, D * n_hidden]\n",
    "        if self.linear:\n",
    "            data = data.reshape(N * L, -1)\n",
    "            data = self.linear(data)\n",
    "            data = data.reshape(N, L, -1)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    This class use for slicing initial image\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels : int,\n",
    "                 img_shape : tuple,\n",
    "                 len_alphabet : int,\n",
    "                 num_layers : int = 5,\n",
    "                 increase_channels_layers : list = [1, 2, 4],\n",
    "                 modules_seq : str = 'CAMB',\n",
    "                 modules_freq : list = [1, 1, 1, 1],\n",
    "                 conv_kernel_size : Union[int, tuple] = 3,\n",
    "                 conv_stride : Union[int, tuple] = 1,\n",
    "                 conv_padding : Union[int, tuple] = 1,\n",
    "                 pool_kernel_size : Union[int, tuple] = 2,\n",
    "                 pool_stride : Union[int, tuple] = 2,\n",
    "                 pool_padding : Union[int, tuple] = 0,\n",
    "                 activation : str = 'ReLU',\n",
    "                 rnn_type : str = 'RNN'\n",
    "                 ) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        out_channels = 64\n",
    "        \n",
    "        # Frequence of append every module. For example:\n",
    "        # if freq for MaxPool is 2 then MaxPool will be appended every 2 layer\n",
    "        frequency = dict(zip(modules_seq, modules_freq)) \n",
    "        \n",
    "        # For every layer create sequence of modules\n",
    "        for layer in range(1, num_layers + 1):\n",
    "            for module in modules_seq:\n",
    "                if layer % frequency[module]: # Check freq of module\n",
    "                    continue\n",
    "                if module == 'C':\n",
    "                    self.layers.append(nn.Conv2d(in_channels, out_channels, conv_kernel_size,\n",
    "                                                conv_stride, conv_padding))\n",
    "                    in_channels = out_channels\n",
    "                    img_shape = self.conv_output_shape(img_shape, conv_kernel_size, conv_stride, conv_padding)\n",
    "                elif module == 'A':\n",
    "                    self.layers.append(getattr(nn, activation)())\n",
    "                elif module == 'M':\n",
    "                    self.layers.append(nn.MaxPool2d(pool_kernel_size, pool_stride, pool_padding))\n",
    "                    img_shape = self.conv_output_shape(img_shape, pool_kernel_size, pool_stride, pool_padding)\n",
    "                elif module == 'B':\n",
    "                    self.layers.append(nn.BatchNorm2d(out_channels))\n",
    "            \n",
    "            if layer in increase_channels_layers:\n",
    "                out_channels *= 2\n",
    "        \n",
    "        self.rnn_layers = nn.Sequential(\n",
    "            RNN(img_shape[0]*512, 512, rnn_type=rnn_type, bidirectional=True, batch_first=True),\n",
    "            RNN(1024, 128, len_alphabet + 1, rnn_type=rnn_type, bidirectional=True, batch_first=True)\n",
    "        )\n",
    "        \n",
    "        print(f'Shape after convs layers: {img_shape}')\n",
    "    \n",
    "    \n",
    "    def conv_output_shape(self,\n",
    "                          h_w : tuple,\n",
    "                          kernel_size : Union[int, tuple] = 1,\n",
    "                          stride : Union[int, tuple] = 1,\n",
    "                          pad : Union[int, tuple] = 0,\n",
    "                          dilation : Union[int, tuple] = 1\n",
    "                          ) -> tuple:\n",
    "        \"\"\"\n",
    "        This method calculate out height and width of img\n",
    "\n",
    "        Args:\n",
    "            h_w (tuple): Input shape.\n",
    "            kernel_size (Union[int, tuple], optional): Defaults to 1.\n",
    "            stride (Union[int, tuple], optional): Defaults to 1.\n",
    "            pad (Union[int, tuple], optional): Defaults to 0.\n",
    "            dilation (Union[int, tuple], optional): _description_. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Output shape\n",
    "        \"\"\"\n",
    "        from math import floor\n",
    "        \n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(pad, int):\n",
    "            pad = (pad, pad)\n",
    "            \n",
    "        h = floor(((h_w[0] + (2 * pad[0]) - (dilation * (kernel_size[0] - 1)) - 1) / stride[0]) + 1)\n",
    "        w = floor(((h_w[1] + (2 * pad[0]) - (dilation * (kernel_size[1] - 1)) - 1) / stride[1]) + 1)\n",
    "        return (h, w)\n",
    "    \n",
    "    \n",
    "    def forward(self, data):\n",
    "        \n",
    "        for module in self.layers:\n",
    "            data = module(data)\n",
    "        \n",
    "        bs, c, h, w = data.shape\n",
    "        \n",
    "        data = data.permute(0, 3, 1, 2).reshape(bs, w, c * h) # Out - bs, w, h * c (N, L, H)\n",
    "        data = self.rnn_layers(data) # Out - N, L, len_alphabet\n",
    "        data = data.permute(1, 0, 2) # Out - L, N, len_alphabet\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(data, 2)\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 optimizer,\n",
    "                 dataloader,\n",
    "                 lossfunc,\n",
    "                 coder,\n",
    "                 epochs,\n",
    "                 model_name,\n",
    "                 train_alphabet,\n",
    "                 scheduler = None,\n",
    "                 logging : bool = False,\n",
    "                 device : str = 'cuda'\n",
    "                 ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.dataloader = dataloader\n",
    "        self.lossfunc = lossfunc\n",
    "        self.coder = coder\n",
    "        self.epochs = epochs\n",
    "        self.model_name = model_name\n",
    "        self.LOGGING = logging\n",
    "        self.DEVICE = device\n",
    "        self.train_alphabet = train_alphabet\n",
    "\n",
    "\n",
    "    def print_epoch_data(self,\n",
    "                         epoch: int,\n",
    "                         mean_loss: float,\n",
    "                         char_error: float,\n",
    "                         word_error: float,\n",
    "                         zero_out_losses: float\n",
    "                         ) -> None:\n",
    "        \"\"\"\n",
    "        This method printing epoch statistics\n",
    "        \"\"\"\n",
    "        \n",
    "        print(tabulate(\n",
    "            [['epoch', 'mean loss', 'mean cer', 'mean wer', 'zero loss warnings'],\n",
    "             [epoch, round(mean_loss, 4), round(char_error, 4),\n",
    "              round(word_error, 4), zero_out_losses]],\n",
    "            headers='firstrow',\n",
    "            tablefmt='fancy_grid'))\n",
    "\n",
    "\n",
    "    def save_model(self, mean_loss: float, char_error: float) -> None:\n",
    "        torch.save(self.model.state_dict(),\n",
    "                    f'./{self.model_name} \\\n",
    "                    _L-{round(mean_loss, 4)} \\\n",
    "                    _CER-{round(char_error, 4)}.pth')\n",
    "\n",
    "\n",
    "    def log(self, mean_loss: float, char_error: float, word_error: float) -> None:\n",
    "        wandb.log({'loss': mean_loss,\n",
    "                    'CER': char_error,\n",
    "                    'WER': word_error,\n",
    "                    'Learn Rate': \n",
    "                        self.scheduler.get_last_lr()[-1] \n",
    "                        if self.scheduler \n",
    "                        else self.optimizer.param_groups[0]['lr']})\n",
    "    \n",
    "    \n",
    "    def print_save_stat(self, outputs: list, epoch: int, zero_out: int) -> None:\n",
    "        \n",
    "        assert len(outputs) != 0, 'Error: bad loss'\n",
    "            \n",
    "        output = torch.Tensor(outputs)\n",
    "        mean_loss = output[:, 0].mean().item()\n",
    "        char_error = output[:, 1].mean().item()\n",
    "        word_error = output[:, 2].mean().item()\n",
    "        \n",
    "        self.print_epoch_data(epoch, mean_loss, char_error, word_error, zero_out)\n",
    "        \n",
    "        if self.LOGGING:\n",
    "            self.log(mean_loss, char_error, word_error)\n",
    "        \n",
    "        if mean_loss < 0.1 or not (epoch + 1) % 5:\n",
    "            self.save_model(mean_loss, char_error)\n",
    "    \n",
    "    def forward(self, data: Tensor, targets: Tensor) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "        classes, lengths = self.coder.encode(targets)\n",
    "        data = data.to(self.DEVICE)\n",
    "        classes = classes.to(self.DEVICE)\n",
    "        \n",
    "        logits = self.model(data)\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, N, C = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T for i in range(N)]).to(self.DEVICE)\n",
    "        classes = classes.view(-1).contiguous()\n",
    "        loss = self.lossfunc(logits, classes, pred_sizes, lengths)\n",
    "    \n",
    "    \n",
    "    def prediction(self, logits: Tensor, pred_sizes: Tensor) -> list[str]:\n",
    "        probs, preds = logits.max(2)\n",
    "        preds = preds.transpose(1, 0).contiguous().view(-1)\n",
    "        sim_preds = self.coder.decode(preds.data, pred_sizes.data)\n",
    "        return sim_preds\n",
    "\n",
    "\n",
    "    def backward(self, loss: Tensor, sim_preds: list, targets: list) -> tuple[float, float]:\n",
    "        CER = CharErrorRate()\n",
    "        WER = WordErrorRate()\n",
    "        cer = CER(sim_preds, targets)\n",
    "        wer = WER(sim_preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        return cer, wer\n",
    "    \n",
    "    \n",
    "    def statistics(self, loss: Tensor, cer: float, wer: float) -> list[float, float, float]:\n",
    "        return [abs(loss.item()), cer, wer]\n",
    "    \n",
    "    \n",
    "    def train(self) -> Model:\n",
    "        self.model.train()\n",
    "        \n",
    "        if self.LOGGING:\n",
    "            wandb.watch(self.model, self.lossfunc, log='all', log_freq=100)\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs), total=self.epochs):\n",
    "            zero_out_losses = 0\n",
    "            outputs = []\n",
    "            for (data, targets) in tqdm(self.dataloader, total=len(self.dataloader)):\n",
    "                \n",
    "                loss, logits, pred_sizes = self.forward(data, targets)\n",
    "                sim_preds = self.prediction(logits, pred_sizes)\n",
    "                cer, wer = self.backward(loss, sim_preds, targets)\n",
    "                outputs.append(self.statistics(loss, cer, wer))\n",
    "            \n",
    "            self.print_save_stat(outputs, epoch, zero_out_losses)\n",
    "        \n",
    "        return self.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Class for evaluate CER, WER of model and\n",
    "    count stat about symbols errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 loader,\n",
    "                 coder,\n",
    "                 device : str = 'cuda'\n",
    "                 ) -> None:\n",
    "        self.model = model.eval()\n",
    "        self.CER = CharErrorRate()\n",
    "        self.WER = WordErrorRate()\n",
    "        self.coder = coder\n",
    "        self.loader = loader\n",
    "        self.device = device\n",
    "        self.avg_matches = 0\n",
    "        \n",
    "        self.original_pred = []\n",
    "        self.original_labels = []\n",
    "        \n",
    "        self.symbol_err = {}\n",
    "        self.length_word_CER = {}\n",
    "        \n",
    "    \n",
    "    def suggest(self, words: list, dictionary: enchant.Dict) -> str:\n",
    "        result = ''\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isalpha():\n",
    "                \n",
    "                # If word is in dict probably it's without errors\n",
    "                cer_suggest = dict()\n",
    "                if dictionary.check(word):\n",
    "                    result += word + ' '\n",
    "                    continue\n",
    "                \n",
    "                # Else dict can suggest what word we need\n",
    "                suggestions = set(dictionary.suggest(word))\n",
    "\n",
    "                # For every suggestion finding CER\n",
    "                for suggest in suggestions:\n",
    "                    if ' ' not in suggest:\n",
    "                        cer = self.CER(suggest, word)\n",
    "                        if cer.item() / len(word) < 0.05:\n",
    "                            cer_suggest[cer] = suggest\n",
    "                \n",
    "                # Get the nearest word\n",
    "                if len(cer_suggest.keys()) > 0: \n",
    "                    result += cer_suggest[min(cer_suggest.keys())] + ' '\n",
    "                # Or take original word if there's no suggestions\n",
    "                else:\n",
    "                    result += word + ' '\n",
    "                    \n",
    "        return result[:-1]\n",
    "\n",
    "    \n",
    "    def word_correction(self, predictions: list[str]) -> list:\n",
    "        correct_predictions = []\n",
    "        dictionary = enchant.Dict(\"ru_RU\")\n",
    "        \n",
    "        # Every pred phrase is splitted by words\n",
    "        # then every word is checked with external dict\n",
    "        for phrase in tqdm(predictions, total=len(predictions)):\n",
    "            words = phrase.split()\n",
    "            result = self.suggest(words, dictionary)\n",
    "            correct_predictions.append(result)\n",
    "            \n",
    "        return correct_predictions\n",
    "    \n",
    "    \n",
    "    def count_errors(self) -> None:\n",
    "        \n",
    "        if not len(self.original_pred):\n",
    "            self.evaluate()\n",
    "        \n",
    "        for pred, true in zip(self.original_pred, self.original_labels):\n",
    "            \n",
    "            # Add CERs for all pairs (pred, true) to collect\n",
    "            # the errors dependence on the length\n",
    "            if len(true) in self.length_word_CER.keys():\n",
    "                self.length_word_CER[len(true)].append(self.CER(pred, true))\n",
    "            else: \n",
    "                self.length_word_CER[len(true)] = [self.CER(pred, true)]\n",
    "            \n",
    "            # Collect pairs of mismatched symbols\n",
    "            if len(true) == len(pred) and true != pred:\n",
    "                for i, j in zip(pred, true):\n",
    "                    if i != j: \n",
    "                        if i in self.symbol_err.keys(): \n",
    "                            if j in self.symbol_err[i].keys():\n",
    "                                self.symbol_err[i][j] += 1\n",
    "                                self.avg_matches += 1\n",
    "                            else: self.symbol_err[i][j] = 1\n",
    "                        else: self.symbol_err[i] = {j : 1}\n",
    "    \n",
    "    \n",
    "    def errors_sym_stat(self) -> tuple[dict[str, dict], dict[str, float]]:\n",
    "    \n",
    "        if not len(self.length_word_CER):\n",
    "            self.count_errors()\n",
    "            self.avg_matches /= len(self.symbol_err.keys())\n",
    "        \n",
    "            # For every error symbol leavy only with number of errors >= self.avg_matches\n",
    "            for pred_sym in self.symbol_err.keys():\n",
    "                self.symbol_err[pred_sym] = dict(filter(\n",
    "                    lambda elem: elem[1] >= self.avg_matches, \n",
    "                    self.symbol_err[pred_sym].items()))\n",
    "                \n",
    "            # Delete empty dictionaries in final stat\n",
    "            self.symbol_err = dict(filter(\n",
    "                lambda elem: len(elem[1]) > 0,\n",
    "                self.symbol_err.items()))\n",
    "\n",
    "            # Count mean value for CER based on length\n",
    "            self.length_word_CER = {\n",
    "                key : torch.Tensor(self.length_word_CER[key]).mean().item()\n",
    "                for key in self.length_word_CER.keys()\n",
    "                }\n",
    "        \n",
    "        return self.symbol_err.copy(), self.length_word_CER.copy()\n",
    "    \n",
    "    \n",
    "    def forward(self, beam_width: int) -> tuple[list[str], list[str]]:\n",
    "        \n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        for iteration, batch in enumerate(tqdm(self.loader)):\n",
    "            data, targets = batch[0].to(self.device), batch[1]\n",
    "            labels.extend(targets)\n",
    "            \n",
    "            logits = self.model(data).contiguous().detach()\n",
    "            T, B, H = logits.size()\n",
    "            pred_sizes = torch.LongTensor([T for i in range(B)])\n",
    "            probs, pos = logits.max(2)\n",
    "            pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "            \n",
    "            if beam_width:\n",
    "                sim_preds = self.coder.beam_decode(logits, B, beam_width)\n",
    "            else:\n",
    "                sim_preds = self.coder.decode(pos.data, pred_sizes.data)\n",
    "                \n",
    "            predictions.extend(sim_preds)\n",
    "        \n",
    "        if not len(self.original_labels):\n",
    "            self.original_pred = predictions\n",
    "            self.original_labels = labels\n",
    "            \n",
    "        return predictions, labels\n",
    "\n",
    "    def evaluate(self, beam_width: int = 0, correcting: bool = False) -> tuple:\n",
    "        \n",
    "        predictions, labels = self.forward(beam_width)\n",
    "        \n",
    "        # Correct predictions if wants\n",
    "        if correcting: \n",
    "            predictions = self.word_correction(predictions)\n",
    "        \n",
    "        # Count CER, WER\n",
    "        char_error = self.CER(predictions, labels)\n",
    "        word_error = self.WER(predictions, labels)\n",
    "        \n",
    "        return char_error, word_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recognizer:\n",
    "    \"\"\"\n",
    "    This class can recognize phrase from painted or given picture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 coder,\n",
    "                 transform: Transforms,\n",
    "                 device = 'cuda') -> None:\n",
    "        self.model = model.eval()\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.coder = coder\n",
    "    \n",
    "    \n",
    "    def forward(self, img: Image, beam_width: int) -> str:\n",
    "        \"\"\"\n",
    "        This method implements forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            img (Image): Given image to recognize.\n",
    "            If Image then transforms must be True. Else it's assumed transforms have already been applied\n",
    "\n",
    "        Returns:\n",
    "            str: Recognized phrase\n",
    "        \"\"\"\n",
    "        img = self.transform(img).unsqueeze(0)\n",
    "\n",
    "        logits = self.model(img.to(self.device))\n",
    "        logits = logits.contiguous().cpu()\n",
    "        T, B, H = logits.size()\n",
    "        pred_sizes = torch.LongTensor([T])\n",
    "        probs, pos = logits.max(2)\n",
    "        pos = pos.transpose(1, 0).contiguous().view(-1)\n",
    "        if beam_width:\n",
    "            prediction = self.coder.beam_decode(logits, B, beam_width)\n",
    "        else:\n",
    "            prediction = self.coder.decode(pos.data, pred_sizes.data)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def paint(self) -> None:\n",
    "        \"\"\"\n",
    "        This method creates a window to paint a phrase\n",
    "        \"\"\"\n",
    "        width = 1000  # canvas width\n",
    "        height = 400 # canvas height\n",
    "        center = height//2\n",
    "        white = (255, 255, 255) # canvas back\n",
    "        \n",
    "        self.master = Tk()\n",
    "\n",
    "        # Create a tkinter canvas to draw on\n",
    "        self.canvas = Canvas(self.master, width=width, height=height, bg='white')\n",
    "        self.canvas.pack()\n",
    "\n",
    "        # Create an empty PIL image and draw object to draw on\n",
    "        self.img = PIL.Image.new(\"RGB\", (width, height), white)\n",
    "        self.draw = ImageDraw.Draw(self.img)\n",
    "        self.canvas.pack(expand=YES, fill=BOTH)\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_img)\n",
    "\n",
    "        # Button to recognize img and close pint window\n",
    "        button=Button(text=\"Recognize\",command=self.master.destroy)\n",
    "        button.pack()\n",
    "        \n",
    "        self.master.mainloop()\n",
    "    \n",
    "\n",
    "    def draw_img(self, event) -> None:\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        self.canvas.create_oval(x1, y1, x2, y2, fill=\"black\",width=5)\n",
    "        self.draw.line([x1, y1, x2, y2],fill=\"black\",width=5)\n",
    "\n",
    "\n",
    "    def recognize_from_painted(self, beam_width: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        This method allows you to paint phrase to recognize\n",
    "\n",
    "        Returns:\n",
    "            str: Recognized phrase\n",
    "        \"\"\"\n",
    "        self.paint()\n",
    "        prediction = self.forward(self.img, beam_width)\n",
    "        return prediction\n",
    "\n",
    "    \n",
    "    def recognize_from_file(self, path, beam_width: int = 0, correcting = False) -> str:\n",
    "        \"\"\"\n",
    "        This method loads img from given then recognize it\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to img\n",
    "            correcting (bool, optional): If you want to correct predicted word with dictionary. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str: Recognized phrase\n",
    "        \"\"\"\n",
    "        img = PIL.Image.open(path)\n",
    "        prediction = self.forward(img, beam_width)\n",
    "        \n",
    "        if correcting: \n",
    "            evaluator = Evaluator(self.model, None, self.coder)\n",
    "            prediction = evaluator.word_correction([prediction])[0]\n",
    "            \n",
    "        return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = './HWTRecognition/config'\n",
    "CONFIG_NAME = 'config'\n",
    "\n",
    "with initialize(version_base=None, config_path=CONFIG_PATH):\n",
    "    cfg = compose(CONFIG_NAME)\n",
    "    cfg = compose(CONFIG_NAME, [f'+transforms={cfg.transforms}',\n",
    "                                f'+model={cfg.model}',\n",
    "                                f'+scheduler={cfg.scheduler}',\n",
    "                                f'+optim={cfg.optim}'])\n",
    "    if cfg.transforms.params.Grayscale.prob > 0:\n",
    "        in_channels = cfg.transforms.params.Grayscale.params.num_output_channels\n",
    "    else:\n",
    "        in_channels = 3\n",
    "    img_shape = cfg.transforms.params.Resize.params.size\n",
    "    with open_dict(cfg):\n",
    "        cfg.model.params.in_channels = in_channels\n",
    "        cfg.model.params.img_shape = img_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.id_resume and cfg.logging:\n",
    "    wandb.init(\n",
    "        id=cfg.id_resume,\n",
    "        project=\"Handwritten text recognition\",\n",
    "        resume='must'\n",
    "    )\n",
    "elif cfg.logging:\n",
    "    wandb.init(\n",
    "        project=\"Handwritten text recognition\",\n",
    "        name = f\"{cfg.model.name}_{cfg.transforms.name}_{cfg.optim.name}_{cfg.scheduler.name}\",\n",
    "        config={\n",
    "            'Model': cfg.model.name,\n",
    "            'Transform': cfg.transforms.name,\n",
    "            'Optimizer': cfg.optim.name,\n",
    "            'Scheduler': cfg.scheduler.name if cfg.scheduler else 'None',\n",
    "            'architecture': 'RCNN',\n",
    "            'dataset': 'Handwritten Cyrillic dataset' if cfg.dataset == 'old_' else 'Custom dataset',\n",
    "            'epochs': cfg.epochs,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transforms(cfg.transforms.params)\n",
    "\n",
    "train_data = HWTDataset(cfg.train.dir,\n",
    "                        cfg.train.labels,\n",
    "                        transform)\n",
    "test_data = HWTDataset(cfg.test.dir,\n",
    "                       cfg.test.labels,\n",
    "                       transform)\n",
    "\n",
    "train_df = pd.read_csv(cfg.train.labels, delimiter='\\t', names = ['Image name', 'Label'])\n",
    "train_alphabet = set(train_df['Label'].to_string()) - set('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after convs layers: (8, 28)\n"
     ]
    }
   ],
   "source": [
    "model = Model(**cfg.model.params)\n",
    "model = model.to(cfg.device)\n",
    "\n",
    "coder = SymbolCoder(train_alphabet)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, cfg.train_batch, True)\n",
    "test_dataloader = DataLoader(test_data, cfg.test_batch, True)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(reduction='mean', zero_infinity=True)\n",
    "optimizer = getattr(torch.optim, cfg.optim.optim)(model.parameters(), **cfg.optim.params)\n",
    "\n",
    "if cfg.scheduler:\n",
    "    if cfg.scheduler.scheduler == 'OneCycleLR':\n",
    "        cfg.scheduler.params.total_steps *= len(train_dataloader)\n",
    "    if cfg.scheduler.scheduler == 'StepLR':\n",
    "        cfg.scheduler.params.step_size *= len(train_dataloader)\n",
    "    scheduler = getattr(torch.optim.lr_scheduler,\n",
    "                        cfg.scheduler.scheduler)(optimizer,\n",
    "                                                    **cfg.scheduler.params)\n",
    "else: scheduler = None\n",
    "\n",
    "trainer = Trainer(model, optimizer, scheduler, train_dataloader, ctc_loss, coder,\n",
    "                  cfg.epochs, f'{cfg.model.name}_{cfg.transforms.name}_{cfg.optim.name}_{cfg.scheduler.name}',\n",
    "                  train_alphabet, cfg.logging, cfg.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): ModuleList(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.01)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): LeakyReLU(negative_slope=0.01)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): LeakyReLU(negative_slope=0.01)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (rnn_layers): Sequential(\n",
       "    (0): RNN(\n",
       "      (rnn): RNN(4096, 512, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "    (1): RNN(\n",
       "      (rnn): RNN(1024, 128, batch_first=True, bidirectional=True)\n",
       "      (linear): Linear(in_features=256, out_features=94, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/M1_T1_O2_S3_L-0.0537_CER-0.0151.pth'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обычная оценка тестовых данных, без коррекции слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(model, test_dataloader, coder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ee166a48c84d31bdda65dd5821402a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0943), tensor(0.3593))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка с beam search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение значения beam_width > 5 практически никак не влияло на результат, поэтому оставил 5, так получилось быстрее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b0eec659b0446dafa1240e1bcae91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0950), tensor(0.3674))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистика неккоректных символов. Видно, что модель предсказывает очень похожие символы. На основе этого можно сделать окончательную коррецию предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'К': {'к': 80},\n",
       " 'о': {'а': 199},\n",
       " 'а': {'о': 125, 'А': 136},\n",
       " 'н': {'к': 104},\n",
       " 'с': {'С': 87},\n",
       " 'ы': {'Ы': 86},\n",
       " 'А': {'а': 101},\n",
       " 'к': {'н': 125, 'К': 73},\n",
       " 'С': {'с': 86}}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.errors_sym_stat()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С проверкой орфографии слов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корректировка предсказаний по словарю. Результат только ухудшился. Нужно или менять словарь, или писать корректировку на основе ошибочных символов (выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ded76e1c37346369231a438fc4cbb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1097678190da4ccaaed324911cc5e7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3060), tensor(0.4560))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(correcting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013888889302810034"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CharErrorRate()('Онакраствая', 'Она красивая').item()/12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рисование слова и его предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer = Recognizer(model, coder, transform, 'cuda')\n",
    "recognizer.recognize_from_painted()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие символы чаще путает наша модель, а также ошибки в зависимости от длины последовательности. На основе этой статистики возможно реализовать замену вероятных ошибочных символов и искать ближайшее совпадение в словаре (с учетом того, что в нашем тексте нет требований к сохранению четкой орфографии писавшего человека). А на основе соотношения длины к ошибке можно изменить модель - увеличить или уменьшить входящую в RNN последовательность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'К': {'к': 80}, 'о': {'а': 199}, 'а': {'о': 125, 'А': 136}, 'н': {'к': 104}, 'с': {'С': 87}, 'ы': {'Ы': 86}, 'А': {'а': 101}, 'к': {'н': 125, 'К': 73}, 'С': {'с': 86}}\n"
     ]
    }
   ],
   "source": [
    "stat = evaluator.errors_sym_stat()\n",
    "print(stat[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на CER в зависимости от длины последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x263484a4dc0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(dict(sorted(stat[1].items())).keys())\n",
    "y = dict(sorted(stat[1].items())).values()\n",
    "\n",
    "plt.ylabel('CER')\n",
    "plt.xlabel('Длина последовательности')\n",
    "plt.xticks(x)\n",
    "plt.scatter(x, y, color = 'red', label = 'CER последовательности')\n",
    "plt.plot(x, [0.095 for i in x], color = 'blue', label = 'CER на всей выборке')\n",
    "plt.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cffdd22708c8f24895f497a03a7b67c0092c0fcb40692f19cd97059e00134830"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
