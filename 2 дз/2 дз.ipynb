{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjM7DFps2rBi"
      },
      "source": [
        "# **Занятие 2.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "alR-VHX_gnQK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_lightning import seed_everything\n",
        "%matplotlib inline\n",
        "\n",
        "seed_everything(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8urvcsAKi62"
      },
      "source": [
        "# Создание собственной библиотеки автоматического дифференцирования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA2PNhudUNij"
      },
      "source": [
        "## Собственное автоматическое дифференцирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Сначала реализовал пример на бумаге, для того, чтобы не запутаться"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image alt](./%D0%B3%D1%80%D0%B0%D1%84%20%D0%B1%D1%8D%D0%BA%D0%B2%D0%B0%D1%80%D0%B4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Также здесь уже добавлена ф-ия Softmax из задания ниже"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "chDdD9oSUlUJ"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data: float, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None # function \n",
        "        self._prev = set(_children) # set of Value objects\n",
        "        self._op = _op # the op that produced this node, string ('+', '-', ....)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data) # Standart expression\n",
        "\n",
        "        def _backward():\n",
        "            # Calculating the derivative of the sum\n",
        "            self.grad += out.grad \n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "        \n",
        "        # Add children to resulting expression\n",
        "        out._prev.add(other)\n",
        "        out._prev.add(self)\n",
        "        \n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data)\n",
        "\n",
        "        def _backward():\n",
        "            # Calculating the derivative of the product\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        # Add children to resulting expression\n",
        "        out._prev.add(other)\n",
        "        out._prev.add(self)\n",
        "        \n",
        "        \n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data ** other)\n",
        "\n",
        "        def _backward():\n",
        "            # Calculating the derivative of a power func\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "        \n",
        "        # Add children to resulting expression\n",
        "        out._prev.add(self)\n",
        "\n",
        "        return out\n",
        "    \n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(self.data) if self.data > 0 else Value(0)\n",
        "\n",
        "        def _backward():\n",
        "            # Calculating the derivative of the ReLU\n",
        "            self.grad += out.grad if out.data > 0 else 0\n",
        "        out._backward = _backward\n",
        "\n",
        "        # Add children to resulting expression\n",
        "        out._prev.add(self)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    \n",
        "    def exp(self):\n",
        "        import math\n",
        "        \n",
        "        out = Value(math.e ** self.data)\n",
        "\n",
        "        def _backward():\n",
        "            # Calculating the derivative of a exp func\n",
        "            self.grad += out.data * out.grad\n",
        "        out._backward = _backward\n",
        "        \n",
        "        # Add children to resulting expression\n",
        "        out._prev.add(self)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    \n",
        "    def softmax(input):\n",
        "        e = [item.exp() for item in input]\n",
        "        s = sum(e)\n",
        "        out = [item / s for item in e]\n",
        "        \n",
        "        return out\n",
        "        \n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in range(len(topo) - 1, -1, -1):\n",
        "            topo[v]._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "    \n",
        "    def __round__(self, n = 0):\n",
        "        return Value(round(self.data, n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vY7OzWjuUiaa"
      },
      "outputs": [],
      "source": [
        "def test_sanity_check():\n",
        "\n",
        "    x = Value(-4.0)\n",
        "    z = 2 * x + 2 + x\n",
        "  \n",
        "    q = z.relu() + z * x\n",
        "    h = (z * z).relu()\n",
        "    y = h + q + q * x\n",
        "    y.backward()\n",
        "    xmg, ymg = x, y\n",
        "\n",
        "    x = torch.Tensor([-4.0]).double()\n",
        "    x.requires_grad = True\n",
        "    z = 2 * x + 2 + x\n",
        "    q = z.relu() + z * x\n",
        "    h = (z * z).relu()\n",
        "    y = h + q + q * x\n",
        "    y.backward()\n",
        "    xpt, ypt = x, y\n",
        "\n",
        "    \n",
        "    # forward pass went well\n",
        "    assert ymg.data == ypt.data.item()\n",
        "    # backward pass went well\n",
        "    print(xmg, xpt, xpt.grad)\n",
        "    assert xmg.grad == xpt.grad.item()\n",
        "\n",
        "\n",
        "def test_more_ops():\n",
        "\n",
        "    a = Value(-4.0)\n",
        "    b = Value(2.0)\n",
        "    c = a + b\n",
        "    d = a * b + b**3\n",
        "    c += c + 1\n",
        "    c += 1 + c + (-a)\n",
        "    d += d * 2 + (b + a).relu()\n",
        "    d += 3 * d + (b - a).relu()\n",
        "    e = c - d\n",
        "    f = e**2\n",
        "    g = f / 2.0\n",
        "    g += 10.0 / f\n",
        "    g.backward()\n",
        "    amg, bmg, gmg = a, b, g\n",
        "\n",
        "    a = torch.Tensor([-4.0]).double()\n",
        "    b = torch.Tensor([2.0]).double()\n",
        "    a.requires_grad = True\n",
        "    b.requires_grad = True\n",
        "    c = a + b\n",
        "    d = a * b + b**3\n",
        "    c = c + c + 1\n",
        "    c = c + 1 + c + (-a)\n",
        "    d = d + d * 2 + (b + a).relu()\n",
        "    d = d + 3 * d + (b - a).relu()\n",
        "    e = c - d\n",
        "    f = e**2\n",
        "    g = f / 2.0\n",
        "    g = g + 10.0 / f\n",
        "    g.backward()\n",
        "    apt, bpt, gpt = a, b, g\n",
        "\n",
        "    tol = 1e-6\n",
        "    # forward pass went well\n",
        "    assert abs(gmg.data - gpt.data.item()) < tol\n",
        "    # backward pass went well\n",
        "    assert abs(amg.grad - apt.grad.item()) < tol\n",
        "    assert abs(bmg.grad - bpt.grad.item()) < tol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w9n8DN6RYkrx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value(data=-4.0, grad=46.0) tensor([-4.], dtype=torch.float64, requires_grad=True) tensor([46.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "test_sanity_check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1T198QDQYh_q"
      },
      "outputs": [],
      "source": [
        "test_more_ops()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-KbDOhMYHZ1"
      },
      "source": [
        "# Обучение на основе собственной бибилотеки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVK1JLXom0Ze"
      },
      "source": [
        "## Многослойный перцептрон на основе класса Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rkl70dxhkcQN"
      },
      "outputs": [],
      "source": [
        "class Module:\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for layer in self.parameters():\n",
        "            for neuron in layer:\n",
        "                for w in neuron:\n",
        "                    w.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        _parameters = []\n",
        "        for param in self.__dict__:\n",
        "            try:\n",
        "                for in_param in getattr(self, param):\n",
        "                    if isinstance(in_param, Module):\n",
        "                        _parameters.append(in_param.parameters())\n",
        "                    elif isinstance(in_param, Value):\n",
        "                        _parameters.append(in_param)\n",
        "            except:\n",
        "                if isinstance(getattr(self, param), Module):\n",
        "                    _parameters.append(getattr(self, param).parameters())\n",
        "                elif isinstance(getattr(self, param), Value):\n",
        "                    _parameters.append(getattr(self, param))\n",
        "        \n",
        "        return _parameters\n",
        "    \n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        self.w = [Value(random.gauss(0, 1)) for _ in range(nin)]\n",
        "        self.b = Value(random.gauss(0, 1))\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum([self.w[i] * x[i] for i in range(len(x))]) + self.b\n",
        "        return act.relu() if self.nonlin else act #act.step() для ступенчатой ф-ии\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "\n",
        "class Layer(Module):\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        self.neurons = [Neuron(nin, kwargs['nonlin']) for i in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = []\n",
        "        for item in x:\n",
        "            res = [neuron(item) for neuron in self.neurons]\n",
        "            out.append(res[0] if len(res) == 1 else res)\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "    \n",
        "class MSELoss(Module):\n",
        "    \n",
        "    def __call__(self, y_true, y_pred):\n",
        "        res = 0\n",
        "        for true, pred in zip(y_true, y_pred):\n",
        "            res += (true - pred)**2\n",
        "        return res / len(y_pred)\n",
        "\n",
        "class MLP(Module):\n",
        "\n",
        "    def __init__(self, nin, nouts, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        sz = [nin]\n",
        "        sz.extend(nouts)\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=(i!=len(nouts)-1)) for i in range(len(nouts))]\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        repr = '\\n'.join(str(layer) for layer in self.layers)\n",
        "        return f\"MLP of [{repr}]\"\n",
        "\n",
        " \n",
        "    def step(self):\n",
        "        for layer in self.layers:\n",
        "            for neuron in layer.neurons:\n",
        "                for w in neuron.w:\n",
        "                    w -= self.learning_rate * w.grad\n",
        "                neuron.b -= self.learning_rate * neuron.b.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkkaE1V1m5i5"
      },
      "source": [
        "## Обучение многослойного перцептрона"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWy-H8eCn2zm"
      },
      "source": [
        "Сам перцептрон"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3La6nRi4m920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)]\n",
            "Layer of [ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4)]\n",
            "Layer of [LinearNeuron(4)]]\n",
            "number of parameters 41\n"
          ]
        }
      ],
      "source": [
        "model = MLP(3, [4, 4, 1], learning_rate = 0.01)\n",
        "print(model)\n",
        "print(\"number of parameters\", len(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvkZVOLcnvqu"
      },
      "source": [
        "Набор данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aLJULsNanpVC"
      },
      "outputs": [],
      "source": [
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Код был запущен несколько раз и были замечены различные результаты испытаний - иногда решение сходилось ко всем приблизительно одинаковым значениям как в примере ниже. Это происходило изза того, что скорость обучения слишком большая чтобы \"поймать\" глобальный минимум."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OuCTaTB8n5l0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 loss 3.8966812573745715, accuracy 0.0%\n",
            "step 1 loss 3.155987972581905, accuracy 25.0%\n",
            "step 2 loss 2.5759983831657722, accuracy 25.0%\n",
            "step 3 loss 2.211519481281905, accuracy 0.0%\n",
            "step 4 loss 1.927228902443592, accuracy 0.0%\n",
            "step 5 loss 1.6925088873525087, accuracy 0.0%\n",
            "step 6 loss 1.4967223750449228, accuracy 0.0%\n",
            "step 7 loss 1.3376829402545805, accuracy 0.0%\n",
            "step 8 loss 1.2236704997866708, accuracy 0.0%\n",
            "step 9 loss 1.1298209637104708, accuracy 0.0%\n",
            "step 10 loss 1.0518095132468426, accuracy 0.0%\n",
            "step 11 loss 1.0023117800227048, accuracy 0.0%\n",
            "step 12 loss 1.0003698848036329, accuracy 0.0%\n",
            "step 13 loss 1.0000591815685813, accuracy 0.0%\n",
            "step 14 loss 1.000009469050973, accuracy 0.0%\n",
            "step 15 loss 1.0000015150481556, accuracy 0.0%\n",
            "step 16 loss 1.0000002424077048, accuracy 0.0%\n",
            "step 17 loss 1.0000000387852328, accuracy 0.0%\n",
            "step 18 loss 1.0000000062056373, accuracy 0.0%\n",
            "step 19 loss 1.000000000992902, accuracy 0.0%\n",
            "[-0.0, -0.0, -0.0, -0.0]\n"
          ]
        }
      ],
      "source": [
        "model = MLP(3, [4, 4, 1], learning_rate = 0.3)\n",
        "loss = MSELoss()\n",
        "\n",
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
        "\n",
        "for k in range(20):\n",
        "    \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    predict = model(xs)\n",
        "\n",
        "    # calculate loss (mean square error)\n",
        "    loss_val = loss(ys, predict)\n",
        "    acc = sum([1 for i in range(len(ys)) if ys[i] == round(predict[i]).data]) / len(ys)\n",
        "    \n",
        "    # backward (zero_grad + backward)\n",
        "    loss_val.backward()\n",
        "    \n",
        "    # update\n",
        "    model.step()\n",
        "    \n",
        "    if k % 1 == 0:\n",
        "        print(f\"step {k} loss {loss_val.data}, accuracy {acc*100}%\")\n",
        "print([round(i.data, 2) for i in predict])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При более тщательном подборе скорости обучения модель все же находит локальный минимум, но все еще не может определить истиное последнее значение. Скорее всего это связано с тем, что генерация весов происходит \"слишком\" случайно, а алгоритм не может выбраться из локального минимума без импульса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 loss 33.86298405833632, accuracy 0.0%\n",
            "step 1 loss 4.81722371318773, accuracy 0.0%\n",
            "step 2 loss 1.7752157942163795, accuracy 0.0%\n",
            "step 3 loss 1.3048714082104031, accuracy 0.0%\n",
            "step 4 loss 1.1009231262400276, accuracy 0.0%\n",
            "step 5 loss 1.0124973300929416, accuracy 0.0%\n",
            "step 6 loss 0.965073163128244, accuracy 0.0%\n",
            "step 7 loss 0.9354672983150997, accuracy 0.0%\n",
            "step 8 loss 0.9139371279924209, accuracy 0.0%\n",
            "step 9 loss 0.8963596835168479, accuracy 25.0%\n",
            "step 10 loss 0.8809641284122156, accuracy 25.0%\n",
            "step 11 loss 0.8669664285716869, accuracy 25.0%\n",
            "step 12 loss 0.8539986441121031, accuracy 25.0%\n",
            "step 13 loss 0.8418701871935692, accuracy 25.0%\n",
            "step 14 loss 0.8304677804685794, accuracy 25.0%\n",
            "step 15 loss 0.8197133818929017, accuracy 25.0%\n",
            "step 16 loss 0.8095463487939902, accuracy 25.0%\n",
            "step 17 loss 0.8005845374983889, accuracy 25.0%\n",
            "step 18 loss 0.7952903440240731, accuracy 25.0%\n",
            "step 19 loss 0.79187986012931, accuracy 25.0%\n",
            "[1.27, -0.41, -0.39, -0.54]\n"
          ]
        }
      ],
      "source": [
        "model = MLP(3, [4, 4, 1], learning_rate = 0.1)\n",
        "loss = MSELoss()\n",
        "\n",
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
        "\n",
        "for k in range(20):\n",
        "    \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    predict = model(xs)\n",
        "\n",
        "    # calculate loss (mean square error)\n",
        "    loss_val = loss(ys, predict)\n",
        "    acc = sum([1 for i in range(len(ys)) if ys[i] == round(predict[i]).data]) / len(ys)\n",
        "    \n",
        "    # backward (zero_grad + backward)\n",
        "    loss_val.backward()\n",
        "    \n",
        "    # update\n",
        "    model.step()\n",
        "    \n",
        "    if k % 1 == 0:\n",
        "        print(f\"step {k} loss {loss_val.data}, accuracy {acc*100}%\")\n",
        "print([round(i.data, 2) for i in predict])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Попробуем вместо обычного рандома инициализировать веса с помощью нормального распределения. Тогда после нескольких попыток получим следующий результат"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 loss 29.918388624301553, accuracy 50.0%\n",
            "step 1 loss 2.790502710119344, accuracy 0.0%\n",
            "step 2 loss 1.0603250016812569, accuracy 50.0%\n",
            "step 3 loss 0.39023987243692004, accuracy 50.0%\n",
            "step 4 loss 0.13439485478025112, accuracy 75.0%\n",
            "step 5 loss 0.046703407909980225, accuracy 100.0%\n",
            "step 6 loss 0.01654572994558693, accuracy 100.0%\n",
            "step 7 loss 0.006098044395967117, accuracy 100.0%\n",
            "step 8 loss 0.002421350122013971, accuracy 100.0%\n",
            "step 9 loss 0.0010848056356952817, accuracy 100.0%\n",
            "step 10 loss 0.0005676501689844975, accuracy 100.0%\n",
            "step 11 loss 0.0003453171282782078, accuracy 100.0%\n",
            "step 12 loss 0.00023490194403016988, accuracy 100.0%\n",
            "step 13 loss 0.00017116130280650483, accuracy 100.0%\n",
            "step 14 loss 0.00012973506481878702, accuracy 100.0%\n",
            "step 15 loss 0.00010074076897744155, accuracy 100.0%\n",
            "step 16 loss 7.962508819328705e-05, accuracy 100.0%\n",
            "step 17 loss 6.394376448344005e-05, accuracy 100.0%\n",
            "step 18 loss 5.2190648554468004e-05, accuracy 100.0%\n",
            "step 19 loss 4.3344324143291185e-05, accuracy 100.0%\n",
            "[1.0, -0.99, -0.99, 0.99]\n"
          ]
        }
      ],
      "source": [
        "model = MLP(3, [4, 4, 1], learning_rate = 0.1)\n",
        "loss = MSELoss()\n",
        "\n",
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
        "\n",
        "for k in range(20):\n",
        "    \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    predict = model(xs)\n",
        "\n",
        "    # calculate loss (mean square error)\n",
        "    loss_val = loss(ys, predict)\n",
        "    acc = sum([1 for i in range(len(ys)) if ys[i] == round(predict[i]).data]) / len(ys)\n",
        "    \n",
        "    # backward (zero_grad + backward)\n",
        "    loss_val.backward()\n",
        "    \n",
        "    # update\n",
        "    model.step()\n",
        "    \n",
        "    if k % 1 == 0:\n",
        "        print(f\"step {k} loss {loss_val.data}, accuracy {acc*100}%\")\n",
        "print([round(i.data, 2) for i in predict])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 loss 0.9011933884730603, accuracy 0.0%\n",
            "step 1 loss 0.6837866585010065, accuracy 0.0%\n",
            "step 2 loss 0.5350544232591083, accuracy 0.0%\n",
            "step 3 loss 0.4198922483330204, accuracy 0.0%\n",
            "step 4 loss 0.33048998026963816, accuracy 50.0%\n",
            "step 5 loss 0.26099000485673035, accuracy 50.0%\n",
            "step 6 loss 0.20687659668129027, accuracy 75.0%\n",
            "step 7 loss 0.16483796847553955, accuracy 75.0%\n",
            "step 8 loss 0.13264408784718007, accuracy 100.0%\n",
            "step 9 loss 0.10731112867263276, accuracy 100.0%\n",
            "step 10 loss 0.09121794680658037, accuracy 100.0%\n",
            "step 11 loss 0.08096749113731268, accuracy 100.0%\n",
            "step 12 loss 0.07231016869308589, accuracy 100.0%\n",
            "step 13 loss 0.06460009429411438, accuracy 100.0%\n",
            "step 14 loss 0.05771896950834098, accuracy 100.0%\n",
            "step 15 loss 0.051576537823210214, accuracy 100.0%\n",
            "step 16 loss 0.04609288479694333, accuracy 100.0%\n",
            "step 17 loss 0.041196818741252327, accuracy 100.0%\n",
            "step 18 loss 0.0368248979153626, accuracy 100.0%\n",
            "step 19 loss 0.03292058058643555, accuracy 100.0%\n",
            "[0.9, -0.78, -1.24, 0.9]\n"
          ]
        }
      ],
      "source": [
        "model = MLP(3, [4, 4, 1], learning_rate = 0.1)\n",
        "loss = MSELoss()\n",
        "\n",
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
        "\n",
        "for k in range(20):\n",
        "    \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    predict = model(xs)\n",
        "\n",
        "    # calculate loss (mean square error)\n",
        "    loss_val = loss(ys, predict)\n",
        "    acc = sum([1 for i in range(len(ys)) if ys[i] == round(predict[i]).data]) / len(ys)\n",
        "    \n",
        "    # backward (zero_grad + backward)\n",
        "    loss_val.backward()\n",
        "    \n",
        "    # update\n",
        "    model.step()\n",
        "    \n",
        "    if k % 1 == 0:\n",
        "        print(f\"step {k} loss {loss_val.data}, accuracy {acc*100}%\")\n",
        "print([round(i.data, 2) for i in predict])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Очевидно, что правильная инициализация весов может существенно снизить затраты на обучение и ускорить этот процесс"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Оригинальный алгоритм библиотеки свойствами, похожими на свойства первоначальных моделей, не обладает. Опять же, скорее всего, это связано именно с инициализацией весов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class q(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(q, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(3, 4)\n",
        "        self.fc2 = torch.nn.Linear(4, 4)\n",
        "        self.fc3 = torch.nn.Linear(4, 1)\n",
        "        self.ac2 = torch.nn.ReLU()\n",
        "        torch.nn.init.normal_(self.fc1.weight)\n",
        "        torch.nn.init.normal_(self.fc2.weight)\n",
        "        torch.nn.init.normal_(self.fc2.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.ac2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.ac2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.6654], grad_fn=<DivBackward0>)\n",
            "tensor([39.5903], grad_fn=<DivBackward0>)\n",
            "tensor([1.6603], grad_fn=<DivBackward0>)\n",
            "tensor([1.2690], grad_fn=<DivBackward0>)\n",
            "tensor([1.0703], grad_fn=<DivBackward0>)\n",
            "tensor([0.9725], grad_fn=<DivBackward0>)\n",
            "tensor([0.9069], grad_fn=<DivBackward0>)\n",
            "tensor([0.8572], grad_fn=<DivBackward0>)\n",
            "tensor([0.8162], grad_fn=<DivBackward0>)\n",
            "tensor([0.7827], grad_fn=<DivBackward0>)\n",
            "tensor([0.7528], grad_fn=<DivBackward0>)\n",
            "tensor([0.7280], grad_fn=<DivBackward0>)\n",
            "tensor([0.6885], grad_fn=<DivBackward0>)\n",
            "tensor([0.6450], grad_fn=<DivBackward0>)\n",
            "tensor([0.5944], grad_fn=<DivBackward0>)\n",
            "tensor([0.5527], grad_fn=<DivBackward0>)\n",
            "tensor([0.5037], grad_fn=<DivBackward0>)\n",
            "tensor([0.4649], grad_fn=<DivBackward0>)\n",
            "tensor([0.4152], grad_fn=<DivBackward0>)\n",
            "tensor([0.3954], grad_fn=<DivBackward0>)\n",
            "tensor([0.3641], grad_fn=<DivBackward0>)\n",
            "tensor([0.3065], grad_fn=<DivBackward0>)\n",
            "tensor([0.2618], grad_fn=<DivBackward0>)\n",
            "tensor([0.2158], grad_fn=<DivBackward0>)\n",
            "tensor([0.1888], grad_fn=<DivBackward0>)\n",
            "tensor([0.1562], grad_fn=<DivBackward0>)\n",
            "tensor([0.1183], grad_fn=<DivBackward0>)\n",
            "tensor([0.0877], grad_fn=<DivBackward0>)\n",
            "tensor([0.0715], grad_fn=<DivBackward0>)\n",
            "tensor([0.0510], grad_fn=<DivBackward0>)\n",
            "tensor([0.0344], grad_fn=<DivBackward0>)\n",
            "tensor([0.0251], grad_fn=<DivBackward0>)\n",
            "tensor([0.0218], grad_fn=<DivBackward0>)\n",
            "tensor([0.0139], grad_fn=<DivBackward0>)\n",
            "tensor([0.0096], grad_fn=<DivBackward0>)\n",
            "tensor([0.0061], grad_fn=<DivBackward0>)\n",
            "tensor([0.0040], grad_fn=<DivBackward0>)\n",
            "tensor([0.0027], grad_fn=<DivBackward0>)\n",
            "tensor([0.0021], grad_fn=<DivBackward0>)\n",
            "tensor([0.0014], grad_fn=<DivBackward0>)\n",
            "tensor([0.0010], grad_fn=<DivBackward0>)\n",
            "tensor([0.0009], grad_fn=<DivBackward0>)\n",
            "tensor([0.0011], grad_fn=<DivBackward0>)\n",
            "tensor([0.0009], grad_fn=<DivBackward0>)\n",
            "tensor([0.0008], grad_fn=<DivBackward0>)\n",
            "tensor([0.0007], grad_fn=<DivBackward0>)\n",
            "tensor([0.0006], grad_fn=<DivBackward0>)\n",
            "tensor([0.0005], grad_fn=<DivBackward0>)\n",
            "tensor([0.0005], grad_fn=<DivBackward0>)\n",
            "tensor([0.0004], grad_fn=<DivBackward0>)\n",
            "tensor([[ 0.9919],\n",
            "        [-1.0361],\n",
            "        [-1.0186],\n",
            "        [ 0.9919]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "m = q()\n",
        "loss_func = torch.nn.MSELoss()\n",
        "sgd = torch.optim.SGD(m.parameters(), lr=0.1)\n",
        "xs = torch.Tensor(xs)\n",
        "ys = torch.Tensor(ys)\n",
        "\n",
        "for epoch in range(50):\n",
        "    \n",
        "    m.zero_grad()\n",
        "        \n",
        "    preds = m.forward(xs)\n",
        "    \n",
        "    loss_value = loss(ys, preds)\n",
        "\n",
        "    loss_value.backward()\n",
        "    \n",
        "    sgd.step()\n",
        "    \n",
        "    accuracy = (preds == ys).float().mean()\n",
        "    print(loss_value)\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4maaWL5yg-f"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yyK39RYo084"
      },
      "source": [
        "### Домашнее задание 1. Доделать практику. Оформить код в три отдельных модуля `autograd`, `nn`, `train`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Модули представлены в папке customtorch. Результаты работы:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image-alt](./%D0%A0%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%20%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D0%B5%D0%B9.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdzPyQ-hylKH"
      },
      "source": [
        "### Домашнее задание 2 (Опционально). Создать свою функцию softmax, наследуемую от `torch.autograd.Function` и имплементировать forward и backward проход. Сравнить со стандартной функцией в Pytorch. \n",
        "[Создание функций](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html) [Софтмакс](https://congyuzhou.medium.com/softmax-3408fb42d55a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Расписал производную. Получилось, что `dPx/dx = Px(1-Px)`, а для `dPx/dy = -Px*Py`. По аналогии расписал остальные и собрал все это в матрицу, для удобного умножения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image-alt](./%D0%9F%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D0%B0%D1%8F%20Softmax.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bGMpj9Pf61n2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class self_Softmax(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        result = torch.e ** input.flatten()\n",
        "        result = result / result.sum()\n",
        "        ctx.save_for_backward(result)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        # First matrix on image\n",
        "        result, = ctx.saved_tensors\n",
        "        result = -result.view(-1, 1).expand(-1, len(result))\n",
        "        \n",
        "        # Second matrix on image\n",
        "        reverse_result = -result.clone().T\n",
        "        diag_elem = reverse_result[np.diag_indices(len(result))] - 1\n",
        "        reverse_result[np.diag_indices(len(result))] = diag_elem\n",
        "        \n",
        "        return grad_output @ (result * reverse_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_tensor_params(*tensors):\n",
        "  for x in tensors:\n",
        "    print('---')\n",
        "    print(f\"data - {x.data}\")\n",
        "    print(f\"grad - {x.grad}\")\n",
        "    print(f\"grad_fn - {x.grad_fn}\")\n",
        "    print(f\"req_grad - {x.requires_grad}\")\n",
        "    print(f\"is_leaf - {x.is_leaf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В качестве конечной функции рассмотрел MSE и просто сумму"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "data - tensor([2., 3., 4.])\n",
            "grad - tensor([-0.0178, -0.0394,  0.0572])\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n",
            "---\n",
            "data - tensor([2., 3., 4.])\n",
            "grad - tensor([-0.0178, -0.0394,  0.0572])\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(0.0357, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1 = torch.tensor([2., 3. ,4.], requires_grad = True)\n",
        "x2 = torch.tensor([2., 3. ,4.], requires_grad = True)\n",
        "y = torch.Tensor([0.25, 0.35, 0.4])\n",
        "\n",
        "sm = torch.nn.Softmax(dim = 0)\n",
        "res = sm(x1)\n",
        "loss = ((res - y)**2).mean()\n",
        "\n",
        "self_sm = self_Softmax.apply\n",
        "self_res = self_sm(x2)\n",
        "self_loss = ((self_res - y)**2).mean()\n",
        "\n",
        "loss.backward()\n",
        "self_loss.backward()\n",
        "\n",
        "show_tensor_params(x1, x2)\n",
        "self_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для суммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "data - tensor([2., 3., 4.])\n",
            "grad - tensor([0., 0., 0.])\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n",
            "---\n",
            "data - tensor([2., 3., 4.])\n",
            "grad - tensor([3.7253e-09, 1.4901e-08, 0.0000e+00])\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(-2.9802e-08, grad_fn=<SumBackward0>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1 = torch.tensor([2., 3. ,4.], requires_grad = True)\n",
        "x2 = torch.tensor([2., 3. ,4.], requires_grad = True)\n",
        "y = torch.Tensor([0.25, 0.35, 0.4])\n",
        "\n",
        "sm = torch.nn.Softmax(dim = 0)\n",
        "res = sm(x1)\n",
        "loss = (res - y).sum()\n",
        "\n",
        "self_sm = self_Softmax.apply\n",
        "self_res = self_sm(x2)\n",
        "self_loss = (self_res - y).sum()\n",
        "\n",
        "loss.backward()\n",
        "self_loss.backward()\n",
        "\n",
        "show_tensor_params(x1, x2)\n",
        "self_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Видно, что результаты совпадают, но из-за точности вычислений в собственном дифференцировании получаются значения отличные от нуля"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VPpRO6H6SHF"
      },
      "source": [
        "### Домашнее задание 3 (Опционально). Добавить функцию софтмакс в собственну библиотеку автоматического дифференцирования. Сравнить с пунктом 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Расписал вручную для отладки кода на примере MSE. `a6`, `a7`, `a8` - вычисленные вероятности для `x`, `y`, `z` соответственно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image-alt](./%D0%93%D1%80%D0%B0%D1%84%20Softmax%20%2B%20MSE.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Добавил две функции ниже в класс Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YJfxtqSphFs"
      },
      "outputs": [],
      "source": [
        "def exp(self):\n",
        "        import math\n",
        "        \n",
        "        out = Value(math.e ** self.data)\n",
        "\n",
        "        def _backward():\n",
        "            # Calculating the derivative of a exp func\n",
        "            self.grad += out.data * out.grad\n",
        "        out._backward = _backward\n",
        "        \n",
        "        # Add children to resulting expression\n",
        "        out._prev.add(self)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    \n",
        "def softmax(input):\n",
        "    e = [item.exp() for item in input]\n",
        "    s = sum(e)\n",
        "    out = [item / s for item in e]\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При проверке можно увидеть, что результаты вычислений совпадают с вычислениями torch при различных конечных функциях"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для суммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value(data=2.0, grad=0.0) Value(data=3.0, grad=0.0) Value(data=4.0, grad=0.0)\n",
            "Value(data=0.9999999999999999, grad=1)\n"
          ]
        }
      ],
      "source": [
        "x = Value(2.0)\n",
        "y = Value(3.0)\n",
        "z = Value(4.0)\n",
        "sm = Value.softmax([x, y, z])\n",
        "ys = [0.25, 0.35, 0.4]\n",
        "s = sum(sm)\n",
        "s.backward()\n",
        "print(x, y, z)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Для MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value(data=2.0, grad=-0.01778124792726957) Value(data=3.0, grad=-0.039410354515245355) Value(data=4.0, grad=0.05719160244251492)\n",
            "Value(data=0.035675025648999506, grad=1)\n"
          ]
        }
      ],
      "source": [
        "x = Value(2.0)\n",
        "y = Value(3.0)\n",
        "z = Value(4.0)\n",
        "sm = Value.softmax([x, y, z])\n",
        "ys = [0.25, 0.35, 0.4]\n",
        "s = 0\n",
        "for item, y_true in zip(sm, ys):\n",
        "    s += (item - y_true) ** 2\n",
        "s /= 3\n",
        "s.backward()\n",
        "print(x, y, z)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Проверка на большее количество значений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value:\n",
            "[Value(data=1, grad=-2.734123913507679e-05), Value(data=-1, grad=-3.36778200831724e-06), Value(data=4, grad=-0.0007446225295809933), Value(data=-7, grad=-7.687511605069202e-09), Value(data=10, grad=0.0007753392382360059)]\n",
            "\n",
            "Torch SM:\n",
            "---\n",
            "data - tensor([ 1., -1.,  4., -7., 10.])\n",
            "grad - tensor([-2.7341e-05, -3.3678e-06, -7.4462e-04, -7.6875e-09,  7.7535e-04])\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n",
            "\n",
            "Custom SM:\n",
            "---\n",
            "data - tensor([ 1., -1.,  4., -7., 10.])\n",
            "grad - tensor([-2.7341e-05, -3.3678e-06, -7.4462e-04, -7.6875e-09,  7.7533e-04])\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n"
          ]
        }
      ],
      "source": [
        "x = [Value(1), Value(-1), Value(4), Value(-7), Value(10)]\n",
        "sm = Value.softmax(x)\n",
        "ys = [0.1, 0.05, 0.3, 0.01, 0.54]\n",
        "s = 0\n",
        "for item, y_true in zip(sm, ys):\n",
        "    s += (item - y_true) ** 2\n",
        "s /= 5\n",
        "s.backward()\n",
        "print('Value:')\n",
        "print([i for i in x])\n",
        "\n",
        "x1 = torch.tensor([1., -1., 4., -7., 10.], requires_grad = True)\n",
        "x2 = torch.tensor([1., -1., 4., -7., 10.], requires_grad = True)\n",
        "y = torch.Tensor(ys)\n",
        "\n",
        "sm = torch.nn.Softmax(dim = 0)\n",
        "res = sm(x1)\n",
        "loss = ((res - y)**2).mean()\n",
        "\n",
        "self_sm = self_Softmax.apply\n",
        "self_res = self_sm(x2)\n",
        "self_loss = ((self_res - y)**2).mean()\n",
        "\n",
        "loss.backward()\n",
        "self_loss.backward()\n",
        "\n",
        "print('\\nTorch SM:')\n",
        "show_tensor_params(x1)\n",
        "\n",
        "print('\\nCustom SM:')\n",
        "show_tensor_params(x2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Все результаты идентичны, что говорит о корректности дифференцирования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRRgw0HNsr_a"
      },
      "source": [
        "### Домашнее задание 4 (Опционально). Добавить визуализацию обучения. Потом мы пройдем более подробно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5AWW52REfn5"
      },
      "source": [
        "https://docs.wandb.ai/guides/integrations/pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekFfy3cWVOIW"
      },
      "source": [
        "https://docs.wandb.ai/ref/python/watch  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G4SOp28ok0o"
      },
      "source": [
        "https://docs.wandb.ai/guides/track/jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lumiR8oykL04"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw3c6P7BkP9b"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udPv0ufwkxOv"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "run = wandb.init(project=\"polynom_learning_\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xtpc9MAUodNs"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRuSrP7JQ00i",
        "1b95Z8u7Q3OL"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "cffdd22708c8f24895f497a03a7b67c0092c0fcb40692f19cd97059e00134830"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
